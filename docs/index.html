<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Erik Bernhardson" />
<meta name="author" content="Trey Jones" />
<meta name="author" content="Mikhail Popov" />
<meta name="author" content="Deb Tankersley" />

<meta name="date" content="2017-08-28" />

<title>Search Relevance Surveys</title>

<script src="index_files/jquery-1.12.4/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="index_files/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="index_files/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="index_files/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="index_files/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="index_files/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="index_files/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="index_files/tocify-1.9.1/jquery.tocify.js"></script>
<script src="index_files/navigation-1.1/tabsets.js"></script>
<script src="index_files/navigation-1.1/codefolding.js"></script>
<link href="index_files/highlightjs-1.1/default.css" rel="stylesheet" />
<script src="index_files/highlightjs-1.1/highlight.js"></script>
<script src="index_files/htmlwidgets-0.9/htmlwidgets.js"></script>
<script src="index_files/datatables-binding-0.2/datatables.js"></script>
<link href="index_files/dt-core-1.10.12/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="index_files/dt-core-1.10.12/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="index_files/dt-core-1.10.12/js/jquery.dataTables.min.js"></script>
<link href="index_files/dt-ext-buttons-1.10.12/css/buttons.dataTables.min.css" rel="stylesheet" />
<script src="index_files/dt-ext-buttons-1.10.12/js/dataTables.buttons.min.js"></script>
<script src="index_files/dt-ext-buttons-1.10.12/js/buttons.flash.min.js"></script>
<script src="index_files/dt-ext-buttons-1.10.12/js/buttons.html5.min.js"></script>
<script src="index_files/dt-ext-buttons-1.10.12/js/buttons.colVis.min.js"></script>
<script src="index_files/dt-ext-buttons-1.10.12/js/buttons.print.min.js"></script>
<link href="index_files/nouislider-7.0.10/jquery.nouislider.min.css" rel="stylesheet" />
<script src="index_files/nouislider-7.0.10/jquery.nouislider.min.js"></script>
<link href="index_files/selectize-0.12.0/selectize.bootstrap3.css" rel="stylesheet" />
<script src="index_files/selectize-0.12.0/selectize.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>


<style type="text/css">
  p.abstract{
    text-align: center;
    font-weight: bold;
  }
  div.abstract{
    margin: auto;
    width: 90%;
  }
</style>

<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>



<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Search Relevance Surveys</h1>
<h3 class="subtitle"><em>Judging With Human Graders &amp; Machine Learning</em></h3>
<h4 class="author"><em><a href='https://meta.wikimedia.org/wiki/User:EBernhardson_(WMF)'>Erik Bernhardson</a></em></h4>
<h4 class="author"><em><a href='https://meta.wikimedia.org/wiki/User:TJones_(WMF)'>Trey Jones</a></em></h4>
<h4 class="author"><em><a href='https://meta.wikimedia.org/wiki/User:MPopov_(WMF)'>Mikhail Popov</a></em></h4>
<h4 class="author"><em><a href='https://meta.wikimedia.org/wiki/User:DTankersley_(WMF)'>Deb Tankersley</a></em></h4>
<h4 class="date"><em>28 August 2017</em></h4>
<div class="abstract">
<p class="abstract">Abstract</p>
<p>Using a curated list of 10 search queries and the English Wikipedia articles that were the top 5 results for each one, we asked randomly selected visitors to those articles whether the article they were on was relevant to the respective search query. Using our own judgement about those articles’ relevance as the gold standard, a summary relevance score computed from users’ responses, and the users’ engagement with the survey, we were able to train models to classify articles as relevant or irrelevant with a remarkably high accuracy for the few data points we had to work with. These methods, combined with more data, would enable us to leverage the opinions of our enormous audience to predict article rankings for search queries at a large scale, which we could then feed into our learning-to-rank project to make searching Wikipedia and other Wikimedia projects better for our users.</p>
</div>

</div>


<style type="text/css">
@import url('https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Sans+Pro|Source+Serif+Pro');
body, p {
  font-family: 'Source Serif Pro', serif;
  font-size: 12pt;
}
pre, code {
  font-family: 'Source Code Pro', monospace;
}
table, tr, td, h1, h2, h3, h4, h5, h6 {
  font-family: 'Source Sans Pro', sans-serif;
}
.caption, caption {
  color: #2c3e50;
  font-size: 10pt;
  width: 90%;
  margin: 5px auto;
  text-align: left;
}
p.abstract {
  font-family: 'Source Sans Pro', sans-serif;
  font-weight: bold;
  font-size: 14pt !important;
}
.footnotes {
  margin-bottom: 80%;
}
</style>
<script type="text/javascript">
$( function() {
  /* Lets the user click on the images to view them in full resolution. */
  $( "img" ).wrap( function() {
    var link = $( '<a/>' );
    link.attr( 'href', $( this ).attr( 'src' ));
    link.attr( 'target', '_blank' );
    return link;
  } );
} );
$("p.abstract").text("Executive Summary");
</script>
<p style="text-align: center;">
<a title="By Github project phacility/phabricator & w:de:User:Perhelion [Apache License 2.0 (http://www.apache.org/licenses/LICENSE-2.0)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3AFavicon-Phabricator-WM.png"><img width="16" alt="Favicon-Phabricator-WM" src="https://upload.wikimedia.org/wikipedia/commons/7/72/Favicon-Phabricator-WM.png"/></a> <a href="https://phabricator.wikimedia.org/T171740", title="T171740">Phabricator ticket</a> | <a title="By The Open Source Initiative [CC BY 2.5 (http://creativecommons.org/licenses/by/2.5)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3AOpen_Source_Initiative_keyhole.svg"><img width="16" alt="Open Source Initiative keyhole" src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Open_Source_Initiative_keyhole.svg/16px-Open_Source_Initiative_keyhole.svg.png"/></a> <a href="https://github.com/wikimedia-research/Discovery-Search-Adhoc-SurveyMVP">Open source analysis</a> | <a title="Font Awesome by Dave Gandy - http://fortawesome.github.com/Font-Awesome [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3ADownload_font_awesome.svg"><img width="16" alt="Download font awesome" src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Download_font_awesome.svg/16px-Download_font_awesome.svg.png"/></a> <a href="https://github.com/wikimedia-research/Discovery-Search-Adhoc-SurveyMVP/blob/master/data">Open data</a>
</p>
<div id="background" class="section level2">
<h2>Background</h2>
<p>We performed a series of tests on English Wikipedia requesting feedback from users about whether the article they were reading was relevant to one of the curated search queries. For our minimum viable product (MVP) test, we hard-coded a list of queries and articles. We also tried different wordings of the relevance question, to assess the impact of each.</p>
<div class="figure">
<img src="figures/example_human_search_relevance_survey.png" alt="Uploaded to Phabricator by Erik Bernhardson (F9161493)" />
<p class="caption">Uploaded to Phabricator by Erik Bernhardson (<a href="https://phabricator.wikimedia.org/F9161493">F9161493</a>)</p>
</div>
<p>For this MVP, the queries were chosen to be about topics for which we could confidently judge an article’s relevance beforehand, such as American pop culture:</p>
<ul>
<li>who is v for vendetta?</li>
<li>star and stripes</li>
<li>block buster</li>
<li>10 items or fewer</li>
<li>sailor soldier tinker spy</li>
<li>how do flowers bloom?</li>
<li>yesterday beetles</li>
<li>search engine</li>
<li>what is a genius iq?</li>
<li>why is a baby goat a kid?</li>
</ul>
<p>For each query, we judged the relevance of the articles that were the top 5 results for the queries at the time (and most are still the top 5 results). The following table shows which pages we asked users about and our judgements:</p>
<table>
<thead>
<tr class="header">
<th align="left">query</th>
<th align="left">article</th>
<th align="left">opinion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">who is v for vendetta?</td>
<td align="left"><a href='https://en.wikipedia.org/wiki/V_for_Vendetta_(film)'>V for Vendetta (film)</a></td>
<td align="left">ok</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/V_for_Vendetta'>V for Vendetta</a></td>
<td align="left">ok</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/List_of_V_for_Vendetta_characters'>List of V for Vendetta characters</a></td>
<td align="left">good</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/V_(comics)'>V (comics)</a></td>
<td align="left">best</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Vendetta_Pro_Wrestling'>Vendetta Pro Wrestling</a></td>
<td align="left">bad</td>
</tr>
<tr class="even">
<td align="left">star and stripes</td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Stars_and_Stripes_Forever_(disambiguation)'>Stars and Stripes Forever (disambiguation)</a></td>
<td align="left">ok</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/The_White_Stripes'>The White Stripes</a></td>
<td align="left">bad</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Tars_and_Stripes'>Tars and Stripes</a></td>
<td align="left">bad</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/The_Stars_and_Stripes_Forever'>The Stars and Stripes Forever</a></td>
<td align="left">ok</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Stripes_(film)'>Stripes (film)</a></td>
<td align="left">bad</td>
</tr>
<tr class="odd">
<td align="left">block buster</td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Blockbuster'>Blockbuster</a></td>
<td align="left">best</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Block_Buster!'>Block Buster!</a></td>
<td align="left">good</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/The_Sweet_(album)'>The Sweet (album)</a></td>
<td align="left">bad</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Block_Busters'>Block Busters</a></td>
<td align="left">ok</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Buster_Keaton'>Buster Keaton</a></td>
<td align="left">bad</td>
</tr>
<tr class="even">
<td align="left">10 items or fewer</td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Fewer_vs._less'>Fewer vs. less</a></td>
<td align="left">ok</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/10-foot_user_interface'>10-foot user interface</a></td>
<td align="left">very bad</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Magic_item_(Dungeons_&_Dragons)'>Magic item (Dungeons &amp; Dragons)</a></td>
<td align="left">very bad</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Item-item_collaborative_filtering'>Item-item collaborative filtering</a></td>
<td align="left">very bad</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Item_47'>Item 47</a></td>
<td align="left">very bad</td>
</tr>
<tr class="odd">
<td align="left">sailor soldier tinker spy</td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Tinker_Tailor_Soldier_Spy'>Tinker Tailor Soldier Spy</a></td>
<td align="left">best</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Tinker,_Tailor'>Tinker, Tailor</a></td>
<td align="left">good</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Blanket_of_Secrecy'>Blanket of Secrecy</a></td>
<td align="left">ok</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/List_of_fictional_double_agents'>List of fictional double agents</a></td>
<td align="left">ok</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Ian_Bannen'>Ian Bannen</a></td>
<td align="left">ok</td>
</tr>
<tr class="even">
<td align="left">how do flowers bloom?</td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Britain_in_Bloom'>Britain in Bloom</a></td>
<td align="left">bad</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Flowers_in_the_Attic_(1987_film)'>Flowers in the Attic (1987 film)</a></td>
<td align="left">very bad</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Flower'>Flower</a></td>
<td align="left">best</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Thymaridas'>Thymaridas</a></td>
<td align="left">very bad</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Flowers_in_the_Attic'>Flowers in the Attic</a></td>
<td align="left">very bad</td>
</tr>
<tr class="odd">
<td align="left">yesterday beetles</td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Private_language_argument'>Private language argument</a></td>
<td align="left">very bad</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Diss_(music)'>Diss (music)</a></td>
<td align="left">very bad</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/How_Do_You_Sleep?_(John_Lennon_song)'>How Do You Sleep? (John Lennon song)</a></td>
<td align="left">very bad</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Maria_Mitchell_Association'>Maria Mitchell Association</a></td>
<td align="left">very bad</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/The_Collected_Stories_of_Philip_K._Dick'>The Collected Stories of Philip K. Dick</a></td>
<td align="left">very bad</td>
</tr>
<tr class="even">
<td align="left">search engine</td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Web_search_engine'>Web search engine</a></td>
<td align="left">best</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/List_of_search_engines'>List of search engines</a></td>
<td align="left">good</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Search_engine_optimization'>Search engine optimization</a></td>
<td align="left">ok</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Search_engine_marketing'>Search engine marketing</a></td>
<td align="left">ok</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Audio_search_engine'>Audio search engine</a></td>
<td align="left">ok</td>
</tr>
<tr class="odd">
<td align="left">what is a genius iq?</td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Genius'>Genius</a></td>
<td align="left">good</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/IQ_classification'>IQ classification</a></td>
<td align="left">best</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Genius_(website)'>Genius (website)</a></td>
<td align="left">bad</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/High_IQ_society'>High IQ society</a></td>
<td align="left">ok</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Social_IQ_score_of_bacteria'>Social IQ score of bacteria</a></td>
<td align="left">bad</td>
</tr>
<tr class="even">
<td align="left">why is a baby goat a kid?</td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Goat'>Goat</a></td>
<td align="left">best</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Super_Why!'>Super Why!</a></td>
<td align="left">very bad</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Barney_&_Friends'>Barney &amp; Friends</a></td>
<td align="left">very bad</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/The_Kids_from_Room_402'>The Kids from Room 402</a></td>
<td align="left">very bad</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><a href='https://en.wikipedia.org/wiki/Oliver_Hardy_filmography'>Oliver Hardy filmography</a></td>
<td align="left">very bad</td>
</tr>
</tbody>
</table>
<p>A user visiting one of those articles might be randomly picked for the survey. There were 4 varieties of questions that we asked:</p>
<ol style="list-style-type: decimal">
<li>Would you click on this page when searching for ‘…’?</li>
<li>If you searched for ‘…’, would this article be a good result?</li>
<li>If you searched for ‘…’, would this article be relevant?</li>
<li>If someone searched for ‘…’, would they want to read this article?</li>
</ol>
<p>(Where … was replaced with the actual query.)</p>
<p>The variations on the questions were so we could assess how the wording/phrasing affected the results.</p>
</div>
<div id="results" class="section level2">
<h2>Results</h2>
<div id="first-test" class="section level3 tabset">
<h3>First Test</h3>
<pre class="r"><code>aggregates_first &lt;- responses_first %&gt;%
  dplyr::group_by(query, article, question, choice) %&gt;%
  dplyr::tally() %&gt;%
  dplyr::ungroup() %&gt;%
  tidyr::spread(choice, n, fill = 0) %&gt;%
  dplyr::mutate(
    total = yes + no,
    score = (yes - no) / (total + 1),
    yes = yes / total,
    no = no / total,
    dismiss = dismiss / (total + dismiss),
    engaged = (total + dismiss) / (total + dismiss + timeout)
  ) %&gt;%
  dplyr::select(-c(total, timeout)) %&gt;%
  tidyr::gather(choice, prop, -c(query, article, question)) %&gt;%
  dplyr::mutate(choice = factor(choice, levels = c(&quot;yes&quot;, &quot;no&quot;, &quot;dismiss&quot;, &quot;engaged&quot;, &quot;score&quot;)))</code></pre>
<div id="summary" class="section level4">
<h4>Summary</h4>
<p>The first test (08/04-08/10) had 0 time delay and presented users with options to answer “Yes”, “No”, “I don’t know”, or dismiss the notification. The notification disappeared after 30 seconds if the user did not interact with it. Due to a bug, the “I don’t know” responses were not recorded for this test. There were 11,056 sessions and 3,016 yes/no responses. 8,703 (73.6%) surveys timed out and 100 surveys were dismissed by the user.</p>
<p><img src="index_files/figure-html/summary_first-1.png" width="1536" /><img src="index_files/figure-html/summary_first-2.png" width="1536" /></p>
<p><a href="#first-test">↑ Top of section</a></p>
</div>
<div id="survey-responses" class="section level4">
<h4>Survey Responses</h4>
<p><img src="index_files/figure-html/datavis_first-1.png" width="1536" /><img src="index_files/figure-html/datavis_first-2.png" width="1536" /><img src="index_files/figure-html/datavis_first-3.png" width="1536" /><img src="index_files/figure-html/datavis_first-4.png" width="1536" /><img src="index_files/figure-html/datavis_first-5.png" width="1536" /><img src="index_files/figure-html/datavis_first-6.png" width="1536" /><img src="index_files/figure-html/datavis_first-7.png" width="1536" /><img src="index_files/figure-html/datavis_first-8.png" width="1536" /><img src="index_files/figure-html/datavis_first-9.png" width="1536" /><img src="index_files/figure-html/datavis_first-10.png" width="1536" /></p>
<p><a href="#first-test">↑ Top of section</a></p>
</div>
<div id="relevance-predictions" class="section level4">
<h4>Relevance Predictions</h4>
<p>We want to be able to categorize articles as relevant/irrelevant based on user’s survey responses. We train a number of classification models using expert opinion as the response and a summary <em>score</em> and <em>engagement</em> as predictors. They are computed as follows:</p>
<p><span class="math display">\[
\text{Score} = \frac{\#\{\text{response: yes}\} - \#\{\text{response: no}\}}{\#\{\text{response: yes}\} + \#\{\text{response: no}\} + 1}
\]</span></p>
<p><span class="math display">\[
\text{Engagement} = \frac{\#\{\text{response: yes/no/dismiss}\}}{\#\{\text{surveys}\}}
\]</span></p>
<p>The classifiers trained are:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Random_forest">random forest</a></li>
<li><a href="https://en.wikipedia.org/wiki/Artificial_neural_network#Backpropagation">backpropagated</a> <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural network</a> with 2 hidden layers having 5 and 3 neurons, respectively</li>
<li><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier">naïve Bayes</a></li>
<li><a href="https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting">gradient-boosted trees</a> (via <a href="https://github.com/dmlc/xgboost">XGBoost</a>)</li>
</ul>
<p>We trained a classifier on each of the 4 questions using the default parameters and a random 70% of the pages as training data and assess its accuracy using a test set from the remaining 30% of the pages. We tried 5-class, 3-class, and 2-class models. The 5-label classification performed the worst due to not enough data per-class, which is why there is a substantial improvement when we grouped combined “very bad” with “bad” and combined “good” with “best” to create the 3 classes. Best performances were with binary (<em>irrelevant</em> = very bad / bad, <em>relevant</em> = ok / good / best) classification. Classifiers trained on responses to questions 1 and 4 had the highest accuracy.</p>
<pre class="r"><code>set.seed(42)
ratings_first &lt;- responses_first %&gt;%
  dplyr::mutate(
    question = as.numeric(factor(question, levels = c(
      &quot;Would you click on this page when searching for &#39;...&#39;?&quot;,
      &quot;If you searched for &#39;...&#39;, would this article be a good result?&quot;,
      &quot;If you searched for &#39;...&#39;, would this article be relevant?&quot;,
      &quot;If someone searched for &#39;...&#39;, would they want to read this article?&quot;
    )))
  ) %&gt;%
  dplyr::group_by(query, article, choice, question) %&gt;%
  dplyr::tally() %&gt;%
  dplyr::ungroup() %&gt;%
  tidyr::spread(choice, n, fill = 0) %&gt;%
  dplyr::mutate(
    total = yes + no + 1,
    engaged = yes + no + dismiss,
    score = (yes - no) / total,
    engagement = (engaged + dismiss) / (total + dismiss + timeout),
    # Normalized versions:
    score_norm = (score - mean(score)) / sd(score),
    engagement_norm = (engagement - mean(engagement)) / sd(engagement)
  ) %&gt;%
  dplyr::left_join(trey, by = c(&quot;query&quot;, &quot;article&quot;)) %&gt;%
  dplyr::mutate(
    irrelevant = as.numeric(opinion %in% c(&quot;very bad&quot;, &quot;bad&quot;)),
    ok_or_better = as.numeric(opinion %in% c(&quot;ok&quot;, &quot;good&quot;, &quot;best&quot;)),
    relevant = as.numeric(opinion %in% c(&quot;good&quot;, &quot;best&quot;)),
    very_bad = as.numeric(opinion == &quot;very bad&quot;),
    bad = as.numeric(opinion == &quot;bad&quot;),
    ok = as.numeric(opinion == &quot;ok&quot;),
    good = as.numeric(opinion == &quot;good&quot;),
    best = as.numeric(opinion == &quot;best&quot;),
    opinion2 = factor(dplyr::case_when(
      opinion %in% c(&quot;very bad&quot;, &quot;bad&quot;) ~ &quot;bad&quot;,
      opinion %in% c(&quot;ok&quot;, &quot;good&quot;, &quot;best&quot;) ~ &quot;ok or better&quot;
    ), levels = c(&quot;bad&quot;, &quot;ok or better&quot;)),
    opinion3 = factor(dplyr::case_when(
      opinion %in% c(&quot;very bad&quot;, &quot;bad&quot;) ~ &quot;bad&quot;,
      opinion %in% c(&quot;good&quot;, &quot;best&quot;) ~ &quot;good&quot;,
      opinion == &quot;ok&quot; ~ &quot;ok&quot;
    ), levels = c(&quot;bad&quot;, &quot;ok&quot;, &quot;good&quot;)),
    opinion5 = factor(opinion, levels = c(&quot;very bad&quot;, &quot;bad&quot;, &quot;ok&quot;, &quot;good&quot;, &quot;best&quot;))
  ) %&gt;%
  split(., .$question) %&gt;%
  lapply(function(df) {
    training_idx &lt;- sample.int(nrow(df), 0.7 * nrow(df), replace = FALSE)
    testing_idx &lt;- setdiff(1:nrow(df), training_idx)
    return(list(train = df[training_idx, ], test = df[testing_idx, ]))
  })</code></pre>
<pre class="r"><code>set.seed(0)
logistic_regression &lt;- dplyr::bind_rows(list(
  &quot;2&quot; = dplyr::bind_rows(lapply(ratings_first, function(question) {
    lr &lt;- glm(opinion2 ~ score + engagement, data = question$train, family = binomial())
    predictions &lt;- predict(lr, question$test[, c(&quot;score&quot;, &quot;engagement&quot;)], type = &quot;response&quot;)
    return(data.frame(accuracy = caret::confusionMatrix(
      factor(predictions &gt; 0.5, c(FALSE, TRUE), levels(question$test$opinion2)),
      reference = question$test$opinion2
    )$overall[&quot;Accuracy&quot;]))
  }), .id = &quot;question&quot;),
  &quot;3&quot; = dplyr::bind_rows(lapply(ratings_first, function(question) {
    lr &lt;- nnet::multinom(
      opinion3 ~ score + engagement,
      data = question$train,
      trace = FALSE
    )
    predictions &lt;- predict(lr, question$test[, c(&quot;score&quot;, &quot;engagement&quot;)])
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion3
    )$overall[&quot;Accuracy&quot;]))
  }), .id = &quot;question&quot;),
  &quot;5&quot; = dplyr::bind_rows(lapply(ratings_first, function(question) {
    lr &lt;- nnet::multinom(
      opinion5 ~ score + engagement,
      data = question$train,
      trace = FALSE
    )
    predictions &lt;- predict(lr, question$test[, c(&quot;score&quot;, &quot;engagement&quot;)])
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion5
    )$overall[&quot;Accuracy&quot;]))
  }), .id = &quot;question&quot;)
), .id = &quot;categories&quot;)

random_forest &lt;- dplyr::bind_rows(list(
  &quot;2&quot; = dplyr::bind_rows(lapply(ratings_first, function(question) {
    rf &lt;- randomForest::randomForest(
      opinion2 ~ score + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions &lt;- predict(rf, question$test[, c(&quot;score&quot;, &quot;engagement&quot;)])
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion2
    )$overall[&quot;Accuracy&quot;]))
  }), .id = &quot;question&quot;),
  &quot;3&quot; = dplyr::bind_rows(lapply(ratings_first, function(question) {
    rf &lt;- randomForest::randomForest(
      opinion3 ~ score + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions &lt;- predict(rf, question$test[, c(&quot;score&quot;, &quot;engagement&quot;)])
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion3
    )$overall[&quot;Accuracy&quot;]))
  }), .id = &quot;question&quot;),
  &quot;5&quot; = dplyr::bind_rows(lapply(ratings_first, function(question) {
    rf &lt;- randomForest::randomForest(
      opinion5 ~ score + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions &lt;- predict(rf, question$test[, c(&quot;score&quot;, &quot;engagement&quot;)])
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion5
    )$overall[&quot;Accuracy&quot;]))
  }), .id = &quot;question&quot;)
), .id = &quot;categories&quot;)

neural_net &lt;- dplyr::bind_rows(list(
  &quot;2&quot; = dplyr::bind_rows(lapply(ratings_first, function(question) {
    nn &lt;- neuralnet::neuralnet(
      irrelevant + ok_or_better ~ score + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions &lt;- factor(
      c(&quot;bad&quot;, &quot;ok or better&quot;)[apply(neuralnet::compute(nn, question$test[, c(&quot;score&quot;, &quot;engagement&quot;)])$net.result, 1, which.max)],
      levels = c(&quot;bad&quot;, &quot;ok or better&quot;)
    )
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion2
    )$overall[&quot;Accuracy&quot;]))
  }), .id = &quot;question&quot;),
  &quot;3&quot; = dplyr::bind_rows(lapply(ratings_first, function(question) {
    nn &lt;- neuralnet::neuralnet(
      irrelevant + ok + relevant ~ score + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions &lt;- factor(
      c(&quot;bad&quot;, &quot;ok&quot;, &quot;good&quot;)[apply(neuralnet::compute(nn, question$test[, c(&quot;score&quot;, &quot;engagement&quot;)])$net.result, 1, which.max)],
      levels = c(&quot;bad&quot;, &quot;ok&quot;, &quot;good&quot;)
    )
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion3
    )$overall[&quot;Accuracy&quot;]))
  }), .id = &quot;question&quot;),
  &quot;5&quot; = dplyr::bind_rows(lapply(ratings_first, function(question) {
    nn &lt;- neuralnet::neuralnet(
      very_bad + bad + ok + good + best ~ score + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions &lt;- factor(
      c(&quot;very bad&quot;, &quot;bad&quot;, &quot;ok&quot;, &quot;good&quot;, &quot;best&quot;)[apply(neuralnet::compute(nn, question$test[, c(&quot;score&quot;, &quot;engagement&quot;)])$net.result, 1, which.max)],
      levels = c(&quot;very bad&quot;, &quot;bad&quot;, &quot;ok&quot;, &quot;good&quot;, &quot;best&quot;)
    )
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion5
    )$overall[&quot;Accuracy&quot;]))
  }), .id = &quot;question&quot;)
), .id = &quot;categories&quot;)

naive_bayes &lt;- dplyr::bind_rows(list(
  &quot;2&quot; = dplyr::bind_rows(lapply(ratings_first, function(question) {
    nb &lt;- e1071::naiveBayes(
      opinion2 ~ score + engagement,
      data = question$train
    )
    predictions &lt;- predict(nb, question$test[, c(&quot;score&quot;, &quot;engagement&quot;)])
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion2
    )$overall[&quot;Accuracy&quot;]))
  }), .id = &quot;question&quot;),
  &quot;3&quot; = dplyr::bind_rows(lapply(ratings_first, function(question) {
    nb &lt;- e1071::naiveBayes(
      opinion3 ~ score + engagement,
      data = question$train
    )
    predictions &lt;- predict(nb, question$test[, c(&quot;score&quot;, &quot;engagement&quot;)])
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion3
    )$overall[&quot;Accuracy&quot;]))
  }), .id = &quot;question&quot;),
  &quot;5&quot; = dplyr::bind_rows(lapply(ratings_first, function(question) {
    nb &lt;- e1071::naiveBayes(
      opinion5 ~ score + engagement,
      data = question$train
    )
    predictions &lt;- predict(nb, question$test[, c(&quot;score&quot;, &quot;engagement&quot;)])
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion5
    )$overall[&quot;Accuracy&quot;]))
  }), .id = &quot;question&quot;)
), .id = &quot;categories&quot;)

gradient_boosted &lt;- dplyr::bind_rows(list(
  &quot;2&quot; = dplyr::bind_rows(lapply(ratings_first, function(question) {
    xgb &lt;- xgboost::xgboost(
      data = as.matrix(question$train[, c(&quot;score&quot;, &quot;engagement&quot;)]),
      label = as.numeric(question$train$opinion2) - 1,
      objective = &quot;binary:logistic&quot;, nrounds = 100, nthread = 4, verbose = 0
    )
    predictions &lt;- predict(xgb, as.matrix(question$test[, c(&quot;score&quot;, &quot;engagement&quot;)]))
    return(data.frame(accuracy = caret::confusionMatrix(
      factor(predictions &gt; 0.5, c(FALSE, TRUE), c(&quot;bad&quot;, &quot;ok or better&quot;)),
      reference = question$test$opinion2
    )$overall[&quot;Accuracy&quot;]))
  }), .id = &quot;question&quot;),
  &quot;3&quot; = dplyr::bind_rows(lapply(ratings_first, function(question) {
    xgb &lt;- xgboost::xgboost(
      data = as.matrix(question$train[, c(&quot;score&quot;, &quot;engagement&quot;)]),
      label = as.numeric(question$train$opinion3) - 1, num_class = 3,
      objective = &quot;multi:softmax&quot;, nrounds = 100, nthread = 4, verbose = 0
    )
    predictions &lt;- predict(xgb, as.matrix(question$test[, c(&quot;score&quot;, &quot;engagement&quot;)]))
    return(data.frame(accuracy = caret::confusionMatrix(
      factor(predictions, 0:2, levels(question$test$opinion3)),
      reference = question$test$opinion3
    )$overall[&quot;Accuracy&quot;]))
  }), .id = &quot;question&quot;),
  &quot;5&quot; = dplyr::bind_rows(lapply(ratings_first, function(question) {
    xgb &lt;- xgboost::xgboost(
      data = as.matrix(question$train[, c(&quot;score&quot;, &quot;engagement&quot;)]),
      label = as.numeric(question$train$opinion5) - 1, num_class = 5,
      objective = &quot;multi:softmax&quot;, nrounds = 100, nthread = 4, verbose = 0
    )
    predictions &lt;- predict(xgb, as.matrix(question$test[, c(&quot;score&quot;, &quot;engagement&quot;)]))
    return(data.frame(accuracy = caret::confusionMatrix(
      factor(predictions, 0:4, levels(question$test$opinion5)),
      reference = question$test$opinion5
    )$overall[&quot;Accuracy&quot;]))
  }), .id = &quot;question&quot;)
), .id = &quot;categories&quot;)</code></pre>
<div id="htmlwidget-d1e531ca0498924168ef" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-d1e531ca0498924168ef">{"x":{"filter":"top","filterHTML":"<tr>\n  <td><\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none; position: absolute; width: 200px;\">\n      <div data-min=\"0.142857142857142\" data-max=\"1\" data-scale=\"15\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n<\/tr>","extensions":["Buttons"],"caption":"<caption style=\"text-align: left\">Accuracy of each classifier for each of the four questions:<br>&nbsp;&nbsp;<strong>(1)<\/strong> Would you click on this page when searching for '...'?<br>&nbsp;&nbsp;<strong>(2)<\/strong> If you searched for '...', would this article be a good result?<br>&nbsp;&nbsp;<strong>(3)<\/strong> If you searched for '...', would this article be relevant?<br>&nbsp;&nbsp;<strong>(4)<\/strong> If someone searched for '...', would they want to read this article?<\/caption>","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60"],["logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost"],["2","2","2","2","3","3","3","3","5","5","5","5","2","2","2","2","3","3","3","3","5","5","5","5","2","2","2","2","3","3","3","3","5","5","5","5","2","2","2","2","3","3","3","3","5","5","5","5","2","2","2","2","3","3","3","3","5","5","5","5"],["1","2","3","4","1","2","3","4","1","2","3","4","1","2","3","4","1","2","3","4","1","2","3","4","1","2","3","4","1","2","3","4","1","2","3","4","1","2","3","4","1","2","3","4","1","2","3","4","1","2","3","4","1","2","3","4","1","2","3","4"],[1,0.857142857142857,0.785714285714286,0.866666666666667,0.866666666666667,0.642857142857143,0.5,0.6,0.4,0.571428571428571,0.214285714285714,0.266666666666667,0.666666666666667,0.857142857142857,0.785714285714286,0.866666666666667,0.533333333333333,0.714285714285714,0.428571428571429,0.6,0.266666666666667,0.357142857142857,0.214285714285714,0.333333333333333,0.666666666666667,0.857142857142857,0.714285714285714,0.866666666666667,0.466666666666667,0.785714285714286,0.571428571428571,0.533333333333333,0.266666666666667,0.571428571428571,0.142857142857143,0.2,0.8,0.857142857142857,0.785714285714286,0.866666666666667,0.866666666666667,0.571428571428571,0.571428571428571,0.533333333333333,0.466666666666667,0.571428571428571,0.285714285714286,0.4,0.866666666666667,0.857142857142857,0.642857142857143,0.866666666666667,0.4,0.714285714285714,0.5,0.6,0.266666666666667,0.285714285714286,0.142857142857143,0.466666666666667]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>classifier<\/th>\n      <th>categories<\/th>\n      <th>question<\/th>\n      <th>accuracy<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"autoWidth":true,"language":{"search":"Filter:"},"order":[[4,"desc"]],"dom":"Bfrtip","buttons":["copy","csv"],"columnDefs":[{"className":"dt-right","targets":4},{"orderable":false,"targets":0}],"orderClasses":false,"orderCellsTop":true,"rowCallback":"function(row, data) {\nDTWidget.formatPercentage(this, row, data, 4, 3);\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script>
<p><a href="#first-test">↑ Top of section</a></p>
</div>
</div>
<div id="second-test" class="section level3 tabset">
<h3>Second Test</h3>
<pre class="r"><code>aggregates_second &lt;- responses_second %&gt;%
  dplyr::group_by(query, article, question, choice) %&gt;%
  dplyr::tally() %&gt;%
  dplyr::ungroup() %&gt;%
  tidyr::spread(choice, n, fill = 0) %&gt;%
  dplyr::mutate(
    total = yes + no + unsure,
    score_A = (yes - no) / (yes + no + 1),
    score_B = (yes - no + unsure / 2) / (total + 1),
    yes = yes / total,
    no = no / total,
    unsure = unsure / total,
    dismiss = dismiss / (total + dismiss),
    engaged = (total + dismiss) / (total + dismiss + timeout)
  ) %&gt;%
  dplyr::select(-c(total, timeout)) %&gt;%
  tidyr::gather(choice, prop, -c(query, article, question)) %&gt;%
  dplyr::mutate(choice = factor(choice, levels = c(&quot;yes&quot;, &quot;no&quot;, &quot;unsure&quot;, &quot;dismiss&quot;, &quot;engaged&quot;, &quot;score_A&quot;, &quot;score_B&quot;)))
temp &lt;- dplyr::left_join(
  tidyr::spread(aggregates_second, choice, prop),
  trey, by = c(&quot;query&quot;, &quot;article&quot;)
)</code></pre>
<div id="summary-1" class="section level4">
<h4>Summary</h4>
<p>The second test (08/11-08/18) <strong>had a 60 second time delay</strong> and presented users with options to answer “Yes”/“No”/“I don’t know” (coded as “unsure”) or dismiss the notification. The notification disappeared after 30 seconds if the user did not interact with it. There were 7,828 sessions and 1,893 yes/no/unsure responses. 6,191 (76.3%) surveys timed out and 26 surveys were dismissed by the user.</p>
<p>With the first test, it was easier to develop a scoring method using just the number of “yes” and “no” responses. With the second test, we had to include the “I don’t know” (coded as “unsure”) responses. Because of this, we came up with two possible scoring systems: <strong>method A</strong> does not count “unsure” responses but does use them to normalize the score; <strong>method B</strong> counts half of “unsure” responses and uses all three possible responses to normalize the score. Classifiers trained on responses to questions 3 and 4 appeared to have the highest accuracy.</p>
<p><span class="math display">\[
\text{Engagement} = \frac{\#\{\text{response: yes/no/unsure/dismiss}\}}{\#\{\text{surveys}\}}
\]</span></p>
<p><img src="index_files/figure-html/dataviz_engagement-1.png" width="1536" /></p>
<p><a href="#second-test">↑ Top of section</a></p>
<div id="scoring-method-a" class="section level5">
<h5>Scoring Method A</h5>
<p><span class="math display">\[
\text{Score} = \frac{\#\{\text{response: yes}\} - \#\{\text{response: no}\}}{\#\{\text{response: yes/no}\} + 1}
\]</span></p>
<p><img src="index_files/figure-html/dataviz_score_A-1.png" width="1536" /></p>
</div>
<div id="scoring-method-b" class="section level5">
<h5>Scoring Method B</h5>
<p><span class="math display">\[
\text{Score} = \frac{\#\{\text{response: yes}\} - \#\{\text{response: no}\} + \#\{\text{response: unsure}\}/2}{\#\{\text{response: yes/no/unsure}\} + 1}
\]</span></p>
<p><img src="index_files/figure-html/dataviz_score_b-1.png" width="1536" /></p>
<p><a href="#second-test">↑ Top of section</a></p>
</div>
</div>
<div id="survey-responses-1" class="section level4">
<h4>Survey Responses</h4>
<p><img src="index_files/figure-html/datavis_second-1.png" width="1536" /><img src="index_files/figure-html/datavis_second-2.png" width="1536" /><img src="index_files/figure-html/datavis_second-3.png" width="1536" /><img src="index_files/figure-html/datavis_second-4.png" width="1536" /><img src="index_files/figure-html/datavis_second-5.png" width="1536" /><img src="index_files/figure-html/datavis_second-6.png" width="1536" /><img src="index_files/figure-html/datavis_second-7.png" width="1536" /><img src="index_files/figure-html/datavis_second-8.png" width="1536" /><img src="index_files/figure-html/datavis_second-9.png" width="1536" /><img src="index_files/figure-html/datavis_second-10.png" width="1536" /></p>
<p><a href="#second-test">↑ Top of section</a></p>
</div>
<div id="relevance-predictions-1" class="section level4">
<h4>Relevance Predictions</h4>
<pre class="r"><code>set.seed(42)
ratings_second &lt;- responses_second %&gt;%
  dplyr::mutate(
    question = as.numeric(factor(question, levels = c(
      &quot;Would you click on this page when searching for &#39;...&#39;?&quot;,
      &quot;If you searched for &#39;...&#39;, would this article be a good result?&quot;,
      &quot;If you searched for &#39;...&#39;, would this article be relevant?&quot;,
      &quot;If someone searched for &#39;...&#39;, would they want to read this article?&quot;
    )))
  ) %&gt;%
  dplyr::group_by(query, article, choice, question) %&gt;%
  dplyr::tally() %&gt;%
  dplyr::ungroup() %&gt;%
  tidyr::spread(choice, n, fill = 0) %&gt;%
  dplyr::mutate(
    total = yes + no + unsure + 1,
    engaged = yes + no + unsure + dismiss,
    score_A = (yes - no) / (yes + no + 1),
    score_B = (yes - no + unsure / 2) / total,
    engagement = (engaged + dismiss) / (total + dismiss + timeout),
    # Normalized versions:
    score_A_norm = (score_A - mean(score_A)) / sd(score_A),
    score_B_norm = (score_B - mean(score_B)) / sd(score_B),
    engagement_norm = (engagement - mean(engagement)) / sd(engagement)
  ) %&gt;%
  dplyr::left_join(trey, by = c(&quot;query&quot;, &quot;article&quot;)) %&gt;%
  dplyr::mutate(
    irrelevant = as.numeric(opinion %in% c(&quot;very bad&quot;, &quot;bad&quot;)),
    ok_or_better = as.numeric(opinion %in% c(&quot;ok&quot;, &quot;good&quot;, &quot;best&quot;)),
    relevant = as.numeric(opinion %in% c(&quot;good&quot;, &quot;best&quot;)),
    very_bad = as.numeric(opinion == &quot;very bad&quot;),
    bad = as.numeric(opinion == &quot;bad&quot;),
    ok = as.numeric(opinion == &quot;ok&quot;),
    good = as.numeric(opinion == &quot;good&quot;),
    best = as.numeric(opinion == &quot;best&quot;),
    opinion2 = factor(dplyr::case_when(
      opinion %in% c(&quot;very bad&quot;, &quot;bad&quot;) ~ &quot;bad&quot;,
      opinion %in% c(&quot;ok&quot;, &quot;good&quot;, &quot;best&quot;) ~ &quot;ok or better&quot;
    ), levels = c(&quot;bad&quot;, &quot;ok or better&quot;)),
    opinion3 = factor(dplyr::case_when(
      opinion %in% c(&quot;very bad&quot;, &quot;bad&quot;) ~ &quot;bad&quot;,
      opinion %in% c(&quot;good&quot;, &quot;best&quot;) ~ &quot;good&quot;,
      opinion == &quot;ok&quot; ~ &quot;ok&quot;
    ), levels = c(&quot;bad&quot;, &quot;ok&quot;, &quot;good&quot;)),
    opinion5 = factor(opinion, levels = c(&quot;very bad&quot;, &quot;bad&quot;, &quot;ok&quot;, &quot;good&quot;, &quot;best&quot;))
  ) %&gt;%
  split(., .$question) %&gt;%
  lapply(function(df) {
    training_idx &lt;- sample.int(nrow(df), 0.7 * nrow(df), replace = FALSE)
    testing_idx &lt;- setdiff(1:nrow(df), training_idx)
    return(list(train = df[training_idx, ], test = df[testing_idx, ]))
  })</code></pre>
<pre class="r"><code>data_frame &lt;- function(...) {
  return(data.frame(..., stringsAsFactors = FALSE))
}
set.seed(0)
logistic_regression &lt;- dplyr::bind_rows(list(
  &quot;2&quot; = dplyr::bind_rows(lapply(ratings_second, function(question) {
    lr_A &lt;- glm(opinion2 ~ score_A + engagement, data = question$train, family = binomial())
    predictions_A &lt;- predict(lr_A, question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)], type = &quot;response&quot;)
    lr_B &lt;- glm(opinion2 ~ score_B + engagement, data = question$train, family = binomial())
    predictions_B &lt;- predict(lr_B, question$test[, c(&quot;score_B&quot;, &quot;engagement&quot;)], type = &quot;response&quot;)
    accuracy &lt;- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        factor(predictions_A &gt; 0.5, c(FALSE, TRUE), levels(question$test$opinion2)),
        reference = question$test$opinion2
      )$overall[&quot;Accuracy&quot;]),
      B = data_frame(accuracy = caret::confusionMatrix(
        factor(predictions_B &gt; 0.5, c(FALSE, TRUE), levels(question$test$opinion2)),
        reference = question$test$opinion2
      )$overall[&quot;Accuracy&quot;])
    )
    return(dplyr::bind_rows(accuracy, .id = &quot;scoring&quot;))
  }), .id = &quot;question&quot;),
  &quot;3&quot; = dplyr::bind_rows(lapply(ratings_second, function(question) {
    lr_A &lt;- nnet::multinom(
      opinion3 ~ score_A + engagement,
      data = question$train,
      trace = FALSE
    )
    predictions_A &lt;- predict(lr_A, question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)])
    lr_B &lt;- nnet::multinom(
      opinion3 ~ score_B + engagement,
      data = question$train,
      trace = FALSE
    )
    predictions_B &lt;- predict(lr_B, question$test[, c(&quot;score_B&quot;, &quot;engagement&quot;)])
    accuracy &lt;- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion3
      )$overall[&quot;Accuracy&quot;]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion3
      )$overall[&quot;Accuracy&quot;])
    )
    return(dplyr::bind_rows(accuracy, .id = &quot;scoring&quot;))
  }), .id = &quot;question&quot;),
  &quot;5&quot; = dplyr::bind_rows(lapply(ratings_second, function(question) {
    lr_A &lt;- nnet::multinom(
      opinion5 ~ score_A + engagement,
      data = question$train,
      trace = FALSE
    )
    predictions_A &lt;- predict(lr_A, question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)])
    lr_B &lt;- nnet::multinom(
      opinion5 ~ score_B + engagement,
      data = question$train,
      trace = FALSE
    )
    predictions_B &lt;- predict(lr_B, question$test[, c(&quot;score_B&quot;, &quot;engagement&quot;)])
    accuracy &lt;- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion5
      )$overall[&quot;Accuracy&quot;]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion5
      )$overall[&quot;Accuracy&quot;])
    )
    return(dplyr::bind_rows(accuracy, .id = &quot;scoring&quot;))
  }), .id = &quot;question&quot;)
), .id = &quot;categories&quot;)

random_forest &lt;- dplyr::bind_rows(list(
  &quot;2&quot; = dplyr::bind_rows(lapply(ratings_second, function(question) {
    rf_A &lt;- randomForest::randomForest(
      opinion2 ~ score_A + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions_A &lt;- predict(rf_A, question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)])
    rf_B &lt;- randomForest::randomForest(
      opinion2 ~ score_B + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions_B &lt;- predict(rf_B, question$test[, c(&quot;score_B&quot;, &quot;engagement&quot;)])
    accuracy &lt;- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion2
      )$overall[&quot;Accuracy&quot;]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion2
      )$overall[&quot;Accuracy&quot;])
    )
    return(dplyr::bind_rows(accuracy, .id = &quot;scoring&quot;))
  }), .id = &quot;question&quot;),
  &quot;3&quot; = dplyr::bind_rows(lapply(ratings_second, function(question) {
    rf_A &lt;- randomForest::randomForest(
      opinion3 ~ score_A + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions_A &lt;- predict(rf_A, question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)])
    rf_B &lt;- randomForest::randomForest(
      opinion3 ~ score_B + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions_B &lt;- predict(rf_B, question$test[, c(&quot;score_B&quot;, &quot;engagement&quot;)])
    accuracy &lt;- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion3
      )$overall[&quot;Accuracy&quot;]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion3
      )$overall[&quot;Accuracy&quot;])
    )
    return(dplyr::bind_rows(accuracy, .id = &quot;scoring&quot;))
  }), .id = &quot;question&quot;),
  &quot;5&quot; = dplyr::bind_rows(lapply(ratings_second, function(question) {
    rf_A &lt;- randomForest::randomForest(
      opinion5 ~ score_A + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions_A &lt;- predict(rf_A, question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)])
    rf_B &lt;- randomForest::randomForest(
      opinion5 ~ score_B + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions_B &lt;- predict(rf_B, question$test[, c(&quot;score_B&quot;, &quot;engagement&quot;)])
    accuracy &lt;- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion5
      )$overall[&quot;Accuracy&quot;]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion5
      )$overall[&quot;Accuracy&quot;])
    )
    return(dplyr::bind_rows(accuracy, .id = &quot;scoring&quot;))
  }), .id = &quot;question&quot;)
), .id = &quot;categories&quot;)

neural_net &lt;- dplyr::bind_rows(list(
  &quot;2&quot; = dplyr::bind_rows(lapply(ratings_second, function(question) {
    nn_A &lt;- neuralnet::neuralnet(
      irrelevant + ok_or_better ~ score_A + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions_A &lt;- factor(
      c(&quot;bad&quot;, &quot;ok or better&quot;)[apply(neuralnet::compute(nn_A, question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)])$net.result, 1, which.max)],
      levels = c(&quot;bad&quot;, &quot;ok or better&quot;)
    )
    nn_B &lt;- neuralnet::neuralnet(
      irrelevant + ok_or_better ~ score_B + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions_B &lt;- factor(
      c(&quot;bad&quot;, &quot;ok or better&quot;)[apply(neuralnet::compute(nn_B, question$test[, c(&quot;score_B&quot;, &quot;engagement&quot;)])$net.result, 1, which.max)],
      levels = c(&quot;bad&quot;, &quot;ok or better&quot;)
    )
    accuracy &lt;- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion2
      )$overall[&quot;Accuracy&quot;]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion2
      )$overall[&quot;Accuracy&quot;])
    )
    return(dplyr::bind_rows(accuracy, .id = &quot;scoring&quot;))
  }), .id = &quot;question&quot;),
  &quot;3&quot; = dplyr::bind_rows(lapply(ratings_second, function(question) {
    nn_A &lt;- neuralnet::neuralnet(
      irrelevant + ok + relevant ~ score_A + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions_A &lt;- factor(
      c(&quot;bad&quot;, &quot;ok&quot;, &quot;good&quot;)[apply(neuralnet::compute(nn_A, question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)])$net.result, 1, which.max)],
      levels = c(&quot;bad&quot;, &quot;ok&quot;, &quot;good&quot;)
    )
    nn_B &lt;- neuralnet::neuralnet(
      irrelevant + ok + relevant ~ score_B + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions_B &lt;- factor(
      c(&quot;bad&quot;, &quot;ok&quot;, &quot;good&quot;)[apply(neuralnet::compute(nn_B, question$test[, c(&quot;score_B&quot;, &quot;engagement&quot;)])$net.result, 1, which.max)],
      levels = c(&quot;bad&quot;, &quot;ok&quot;, &quot;good&quot;)
    )
    accuracy &lt;- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion3
      )$overall[&quot;Accuracy&quot;]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion3
      )$overall[&quot;Accuracy&quot;])
    )
    return(dplyr::bind_rows(accuracy, .id = &quot;scoring&quot;))
  }), .id = &quot;question&quot;),
  &quot;5&quot; = dplyr::bind_rows(lapply(ratings_second, function(question) {
    nn_A &lt;- neuralnet::neuralnet(
      very_bad + bad + ok + good + best ~ score_A + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions_A &lt;- factor(
      c(&quot;very bad&quot;, &quot;bad&quot;, &quot;ok&quot;, &quot;good&quot;, &quot;best&quot;)[apply(neuralnet::compute(nn_A, question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)])$net.result, 1, which.max)],
      levels = c(&quot;very bad&quot;, &quot;bad&quot;, &quot;ok&quot;, &quot;good&quot;, &quot;best&quot;)
    )
    nn_B &lt;- neuralnet::neuralnet(
      very_bad + bad + ok + good + best ~ score_B + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions_B &lt;- factor(
      c(&quot;very bad&quot;, &quot;bad&quot;, &quot;ok&quot;, &quot;good&quot;, &quot;best&quot;)[apply(neuralnet::compute(nn_B, question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)])$net.result, 1, which.max)],
      levels = c(&quot;very bad&quot;, &quot;bad&quot;, &quot;ok&quot;, &quot;good&quot;, &quot;best&quot;)
    )
    accuracy &lt;- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion5
      )$overall[&quot;Accuracy&quot;]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion5
      )$overall[&quot;Accuracy&quot;])
    )
    return(dplyr::bind_rows(accuracy, .id = &quot;scoring&quot;))
  }), .id = &quot;question&quot;)
), .id = &quot;categories&quot;)

naive_bayes &lt;- dplyr::bind_rows(list(
  &quot;2&quot; = dplyr::bind_rows(lapply(ratings_second, function(question) {
    nb_A &lt;- e1071::naiveBayes(
      opinion2 ~ score_A + engagement,
      data = question$train
    )
    predictions_A &lt;- predict(nb_A, question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)])
    nb_B &lt;- e1071::naiveBayes(
      opinion2 ~ score_B + engagement,
      data = question$train
    )
    predictions_B &lt;- predict(nb_B, question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)])
    accuracy &lt;- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion2
      )$overall[&quot;Accuracy&quot;]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion2
      )$overall[&quot;Accuracy&quot;])
    )
    return(dplyr::bind_rows(accuracy, .id = &quot;scoring&quot;))
  }), .id = &quot;question&quot;),
  &quot;3&quot; = dplyr::bind_rows(lapply(ratings_second, function(question) {
    nb_A &lt;- e1071::naiveBayes(
      opinion3 ~ score_A + engagement,
      data = question$train
    )
    predictions_A &lt;- predict(nb_A, question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)])
    nb_B &lt;- e1071::naiveBayes(
      opinion3 ~ score_B + engagement,
      data = question$train
    )
    predictions_B &lt;- predict(nb_B, question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)])
    accuracy &lt;- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion3
      )$overall[&quot;Accuracy&quot;]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion3
      )$overall[&quot;Accuracy&quot;])
    )
    return(dplyr::bind_rows(accuracy, .id = &quot;scoring&quot;))
  }), .id = &quot;question&quot;),
  &quot;5&quot; = dplyr::bind_rows(lapply(ratings_second, function(question) {
    nb_A &lt;- e1071::naiveBayes(
      opinion5 ~ score_A + engagement,
      data = question$train
    )
    predictions_A &lt;- predict(nb_A, question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)])
    nb_B &lt;- e1071::naiveBayes(
      opinion5 ~ score_B + engagement,
      data = question$train
    )
    predictions_B &lt;- predict(nb_B, question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)])
    accuracy &lt;- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion5
      )$overall[&quot;Accuracy&quot;]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion5
      )$overall[&quot;Accuracy&quot;])
    )
    return(dplyr::bind_rows(accuracy, .id = &quot;scoring&quot;))
  }), .id = &quot;question&quot;)
), .id = &quot;categories&quot;)

gradient_boosted &lt;- dplyr::bind_rows(list(
  &quot;2&quot; = dplyr::bind_rows(lapply(ratings_second, function(question) {
    xgb_A &lt;- xgboost::xgboost(
      data = as.matrix(question$train[, c(&quot;score_A&quot;, &quot;engagement&quot;)]),
      label = as.numeric(question$train$opinion2) - 1,
      objective = &quot;binary:logistic&quot;, nrounds = 100, nthread = 4, verbose = 0
    )
    predictions_A &lt;- predict(xgb_A, as.matrix(question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)]))
    xgb_B &lt;- xgboost::xgboost(
      data = as.matrix(question$train[, c(&quot;score_B&quot;, &quot;engagement&quot;)]),
      label = as.numeric(question$train$opinion2) - 1,
      objective = &quot;binary:logistic&quot;, nrounds = 100, nthread = 4, verbose = 0
    )
    predictions_B &lt;- predict(xgb_B, as.matrix(question$test[, c(&quot;score_B&quot;, &quot;engagement&quot;)]))
    accuracy &lt;- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        factor(predictions_A &gt; 0.5, c(FALSE, TRUE), c(&quot;bad&quot;, &quot;ok or better&quot;)),
        reference = question$test$opinion2
      )$overall[&quot;Accuracy&quot;]),
      B = data_frame(accuracy = caret::confusionMatrix(
        factor(predictions_B &gt; 0.5, c(FALSE, TRUE), c(&quot;bad&quot;, &quot;ok or better&quot;)),
        reference = question$test$opinion2
      )$overall[&quot;Accuracy&quot;])
    )
    return(dplyr::bind_rows(accuracy, .id = &quot;scoring&quot;))
  }), .id = &quot;question&quot;),
  &quot;3&quot; = dplyr::bind_rows(lapply(ratings_second, function(question) {
    xgb_A &lt;- xgboost::xgboost(
      data = as.matrix(question$train[, c(&quot;score_A&quot;, &quot;engagement&quot;)]),
      label = as.numeric(question$train$opinion3) - 1, num_class = 3,
      objective = &quot;multi:softmax&quot;, nrounds = 100, nthread = 4, verbose = 0
    )
    predictions_A &lt;- predict(xgb_A, as.matrix(question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)]))
    xgb_B &lt;- xgboost::xgboost(
      data = as.matrix(question$train[, c(&quot;score_B&quot;, &quot;engagement&quot;)]),
      label = as.numeric(question$train$opinion3) - 1, num_class = 3,
      objective = &quot;multi:softmax&quot;, nrounds = 100, nthread = 4, verbose = 0
    )
    predictions_B &lt;- predict(xgb_B, as.matrix(question$test[, c(&quot;score_B&quot;, &quot;engagement&quot;)]))
    accuracy &lt;- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        factor(predictions_A, 0:2, levels(question$test$opinion3)),
        reference = question$test$opinion3
      )$overall[&quot;Accuracy&quot;]),
      B = data_frame(accuracy = caret::confusionMatrix(
        factor(predictions_B, 0:2, levels(question$test$opinion3)),
        reference = question$test$opinion3
      )$overall[&quot;Accuracy&quot;])
    )
    return(dplyr::bind_rows(accuracy, .id = &quot;scoring&quot;))
  }), .id = &quot;question&quot;),
  &quot;5&quot; = dplyr::bind_rows(lapply(ratings_second, function(question) {
    xgb_A &lt;- xgboost::xgboost(
      data = as.matrix(question$train[, c(&quot;score_A&quot;, &quot;engagement&quot;)]),
      label = as.numeric(question$train$opinion5) - 1, num_class = 5,
      objective = &quot;multi:softmax&quot;, nrounds = 100, nthread = 4, verbose = 0
    )
    predictions_A &lt;- predict(xgb_A, as.matrix(question$test[, c(&quot;score_A&quot;, &quot;engagement&quot;)]))
    xgb_B &lt;- xgboost::xgboost(
      data = as.matrix(question$train[, c(&quot;score_B&quot;, &quot;engagement&quot;)]),
      label = as.numeric(question$train$opinion5) - 1, num_class = 5,
      objective = &quot;multi:softmax&quot;, nrounds = 100, nthread = 4, verbose = 0
    )
    predictions_B &lt;- predict(xgb_B, as.matrix(question$test[, c(&quot;score_B&quot;, &quot;engagement&quot;)]))
    accuracy &lt;- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        factor(predictions_A, 0:4, levels(question$test$opinion5)),
        reference = question$test$opinion5
      )$overall[&quot;Accuracy&quot;]),
      B = data_frame(accuracy = caret::confusionMatrix(
        factor(predictions_B, 0:4, levels(question$test$opinion5)),
        reference = question$test$opinion5
      )$overall[&quot;Accuracy&quot;])
    )
    return(dplyr::bind_rows(accuracy, .id = &quot;scoring&quot;))
  }), .id = &quot;question&quot;)
), .id = &quot;categories&quot;)</code></pre>
<div id="htmlwidget-db51fb8a64e02acf5c31" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-db51fb8a64e02acf5c31">{"x":{"filter":"top","filterHTML":"<tr>\n  <td><\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"character\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n  <\/td>\n  <td data-type=\"number\" style=\"vertical-align: top;\">\n    <div class=\"form-group has-feedback\" style=\"margin-bottom: auto;\">\n      <input type=\"search\" placeholder=\"All\" class=\"form-control\" style=\"width: 100%;\"/>\n      <span class=\"glyphicon glyphicon-remove-circle form-control-feedback\"><\/span>\n    <\/div>\n    <div style=\"display: none; position: absolute; width: 200px;\">\n      <div data-min=\"0.133333333333333\" data-max=\"0.857142857142858\" data-scale=\"15\"><\/div>\n      <span style=\"float: left;\"><\/span>\n      <span style=\"float: right;\"><\/span>\n    <\/div>\n  <\/td>\n<\/tr>","extensions":["Buttons"],"caption":"<caption style=\"text-align: left\">Accuracy of each classifier for each of the four questions:<br>&nbsp;&nbsp;<strong>(1)<\/strong> Would you click on this page when searching for '...'?<br>&nbsp;&nbsp;<strong>(2)<\/strong> If you searched for '...', would this article be a good result?<br>&nbsp;&nbsp;<strong>(3)<\/strong> If you searched for '...', would this article be relevant?<br>&nbsp;&nbsp;<strong>(4)<\/strong> If someone searched for '...', would they want to read this article?<\/caption>","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120"],["logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","logistic regression","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","random forest","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","neural net (5, 3)","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","naive bayes","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost","xgboost"],["2","2","2","2","2","2","2","2","3","3","3","3","3","3","3","3","5","5","5","5","5","5","5","5","2","2","2","2","2","2","2","2","3","3","3","3","3","3","3","3","5","5","5","5","5","5","5","5","2","2","2","2","2","2","2","2","3","3","3","3","3","3","3","3","5","5","5","5","5","5","5","5","2","2","2","2","2","2","2","2","3","3","3","3","3","3","3","3","5","5","5","5","5","5","5","5","2","2","2","2","2","2","2","2","3","3","3","3","3","3","3","3","5","5","5","5","5","5","5","5"],["1","1","2","2","3","3","4","4","1","1","2","2","3","3","4","4","1","1","2","2","3","3","4","4","1","1","2","2","3","3","4","4","1","1","2","2","3","3","4","4","1","1","2","2","3","3","4","4","1","1","2","2","3","3","4","4","1","1","2","2","3","3","4","4","1","1","2","2","3","3","4","4","1","1","2","2","3","3","4","4","1","1","2","2","3","3","4","4","1","1","2","2","3","3","4","4","1","1","2","2","3","3","4","4","1","1","2","2","3","3","4","4","1","1","2","2","3","3","4","4"],["A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B","A","B"],[0.733333333333333,0.666666666666667,0.5,0.714285714285714,0.714285714285714,0.785714285714286,0.8,0.8,0.4,0.6,0.357142857142857,0.357142857142857,0.642857142857143,0.642857142857143,0.466666666666667,0.333333333333333,0.333333333333333,0.333333333333333,0.285714285714286,0.357142857142857,0.214285714285714,0.285714285714286,0.2,0.2,0.733333333333333,0.8,0.571428571428571,0.785714285714286,0.857142857142857,0.785714285714286,0.6,0.6,0.6,0.533333333333333,0.428571428571429,0.5,0.571428571428571,0.5,0.4,0.333333333333333,0.466666666666667,0.333333333333333,0.357142857142857,0.214285714285714,0.357142857142857,0.357142857142857,0.133333333333333,0.2,0.733333333333333,0.733333333333333,0.5,0.785714285714286,0.571428571428571,0.571428571428571,0.533333333333333,0.6,0.733333333333333,0.533333333333333,0.357142857142857,0.357142857142857,0.5,0.357142857142857,0.4,0.466666666666667,0.266666666666667,0.266666666666667,0.285714285714286,0.214285714285714,0.428571428571429,0.357142857142857,0.266666666666667,0.533333333333333,0.733333333333333,0.466666666666667,0.571428571428571,0.714285714285714,0.785714285714286,0.428571428571429,0.8,0.533333333333333,0.466666666666667,0.533333333333333,0.5,0.428571428571429,0.642857142857143,0.357142857142857,0.466666666666667,0.333333333333333,0.266666666666667,0.266666666666667,0.214285714285714,0.285714285714286,0.285714285714286,0.214285714285714,0.2,0.266666666666667,0.733333333333333,0.8,0.571428571428571,0.785714285714286,0.857142857142857,0.857142857142857,0.733333333333333,0.666666666666667,0.533333333333333,0.533333333333333,0.428571428571429,0.5,0.571428571428571,0.571428571428571,0.4,0.4,0.333333333333333,0.4,0.428571428571429,0.357142857142857,0.285714285714286,0.285714285714286,0.2,0.266666666666667]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>classifier<\/th>\n      <th>categories<\/th>\n      <th>question<\/th>\n      <th>scoring<\/th>\n      <th>accuracy<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":10,"autoWidth":true,"language":{"search":"Filter:"},"order":[[5,"desc"]],"dom":"Bfrtip","buttons":["copy","csv"],"columnDefs":[{"className":"dt-right","targets":5},{"orderable":false,"targets":0}],"orderClasses":false,"orderCellsTop":true,"rowCallback":"function(row, data) {\nDTWidget.formatPercentage(this, row, data, 5, 3);\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script>
<p><a href="#second-test">↑ Top of section</a></p>
</div>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>Using relatively few article judgements, we are able to train models that perform remarkably well given the amount of data. That is, using a larger set of articles for which we have an “expert opinion” regarding their relevance to a certain search query, we will be able to train a model that accurately predicts an article’s relevance just from users’ survey responses and engagement with the survey. This will enable us to use aggregated public opinion to rank a large volume of articles.</p>
<p>Using binary classification (irrelevant/relevant), we can ask the algorithms to return predicted probabilities instead of predicted classes. Assuming the model has exceptional accuracy, we can then feed queries and rankings (predicted from users’ survey responses) as training data into our <a href="https://en.wikipedia.org/wiki/Learning_to_rank">learning-to-rank (LTR)</a> endeavor.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p>
<p>In a presentation of this work,<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> <a href="https://meta.wikimedia.org/wiki/User:Dario_(WMF)">Dario Taraborelli</a> (Head of <a href="https://www.mediawiki.org/wiki/Wikimedia_Research">Wikimedia Research</a>) brought up the valid point of how the question wording (“relevant to you” vs. “relevant to people”) primes the respondent differently and could yield different results. It was interesting, then, to see question 4 (“If <em><strong>someone</strong></em> searched for ‘…’, would <em><strong>they</strong></em> want to read this article?”) show up in both tests as the question whose responses yielded the better classification performances.</p>
</div>
<div id="discussion" class="section level2">
<h2>Discussion</h2>
<p>As this was a proof of concept / MVP, we did not apply a lot of rigor to certain aspects. For example, we did not have multiple experts agree on the articles’ relevance, but rather one person’s opinions were used as the gold standard. On the analysis side, a lot of the time was spent getting multiple classification algorithms running in different configuration combinations; so we were not able to include a more rigorous accuracy estimation approach like cross-validation. At the time of this report’s publication we are planning on launching a third test (<a href="https://phabricator.wikimedia.org/T174106">T174106</a>).</p>
<p>Then there’s the issue of what this looks like in production. For deploying this on Wikipedia, we might want to specifically optimize for encyclopedic searches. For example, “barack obama birthdate” and “iphone 7 release date” are encyclopedic search queries, but “how do i 3d print a miniature santa claus?” is not. Unless we utilize natural language processing and machine learning to detect such queries, we cannot automate query selection by picking the most popular queries to ask users about. We would also need to decide whether to train relevance models on a per-wiki/project/language basis. That is, we cannot assume that the relationship between survey responses, engagement, and relevance is the same on German Wikipedia, French Wikisource, and Wikidata.</p>
<p>Filtering for encyclopedicity is just one thing that human reviewers will have to do. We will have to curate the queries anyway because we need to filter for personally identifiable information (PII), gibberish, and queries in the wrong language. However, although manually reviewing 500 queries takes a few hours, we can easily leverage that into 25K-50K crowd-sourced relevance judgements (50-100 results per query) in a relatively short time, whereas the more tedious <a href="https://www.mediawiki.org/wiki/Discernatron">Discernatron</a> has only gotten ~7500 (150 queries * ~50 results per query) over the course of about a year.</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://www.mediawiki.org/wiki/Wikimedia_Engineering/June_2017_changes/Update_on_Discovery">Wikimedia Engineering/June 2017 changes/Update on Discovery</a><a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Links for <a href="https://phabricator.wikimedia.org/T171740#3549858">notes from Research Group meeting on 24 August 2017</a> and the <a href="https://docs.google.com/a/wikimedia.org/presentation/d/1PuOOSukPYFGWikppGmw9Cg85fay-IT9Fi8gB8-CUda8/edit?usp=sharing">slide deck presented</a><a href="#fnref2">↩</a></p></li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://tools-static.wmflabs.org/cdnjs/ajax/libs/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
