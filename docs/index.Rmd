---
title: "Search Relevance Surveys"
subtitle: "Judging With Human Graders & Machine Learning"
author:
  - "<a href='https://meta.wikimedia.org/wiki/User:EBernhardson_(WMF)'>Erik Bernhardson</a>"
  - "<a href='https://meta.wikimedia.org/wiki/User:TJones_(WMF)'>Trey Jones</a>"
  - "<a href='https://meta.wikimedia.org/wiki/User:MPopov_(WMF)'>Mikhail Popov</a>"
  - "<a href='https://meta.wikimedia.org/wiki/User:DTankersley_(WMF)'>Deb Tankersley</a>"
date: "`r format(Sys.Date(), '%d %B %Y')`"
abstract: >
  Using a curated list of 10 search queries and the English Wikipedia articles that were the top 5 results for each one, we asked randomly selected visitors to those articles whether the article they were on was relevant to the respective search query. Using our own judgement about those articles' relevance as the gold standard, a summary relevance score computed from users' responses, and the users' engagement with the survey, we were able to train models to classify articles as relevant or irrelevant with a remarkably high accuracy for the few data points we had to work with. These methods, combined with more data, would enable us to leverage the opinions of our enormous audience to predict article rankings for search queries at a large scale, which we could then feed into our learning-to-rank project to make searching Wikipedia and other Wikimedia projects better for our users.
output:
  html_document:
    # Table of Contents
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
    toc_depth: 3
    code_folding: hide
    # Figures
    fig_width: 16
    fig_height: 8
    # Theme
    theme: readable
    # Files
    self_contained: false
    keep_md: false
    # Extras
    mathjax: https://tools-static.wmflabs.org/cdnjs/ajax/libs/mathjax/2.6.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML
    md_extensions: +raw_html +markdown_in_html_blocks +tex_math_dollars +fancy_lists +startnum +lists_without_preceding_blankline
---
```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(magrittr)
library(glue)
library(ggplot2)
```
```{css, echo=FALSE}
@import url('https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Sans+Pro|Source+Serif+Pro');
body, p {
  font-family: 'Source Serif Pro', serif;
  font-size: 12pt;
}
pre, code {
  font-family: 'Source Code Pro', monospace;
}
table, tr, td, h1, h2, h3, h4, h5, h6 {
  font-family: 'Source Sans Pro', sans-serif;
}
.caption, caption {
  color: #2c3e50;
  font-size: 10pt;
  width: 90%;
  margin: 5px auto;
  text-align: left;
}
p.abstract {
  font-family: 'Source Sans Pro', sans-serif;
  font-weight: bold;
  font-size: 14pt !important;
}
.footnotes {
  margin-bottom: 80%;
}
```
```{js, echo=FALSE}
$( function() {
  /* Lets the user click on the images to view them in full resolution. */
  $( "img" ).wrap( function() {
    var link = $( '<a/>' );
    link.attr( 'href', $( this ).attr( 'src' ));
    link.attr( 'target', '_blank' );
    return link;
  } );
} );
$("p.abstract").text("Executive Summary");
```
<p style="text-align: center;"><a title="By Github project phacility/phabricator & w:de:User:Perhelion [Apache License 2.0 (http://www.apache.org/licenses/LICENSE-2.0)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3AFavicon-Phabricator-WM.png"><img width="16" alt="Favicon-Phabricator-WM" src="https://upload.wikimedia.org/wikipedia/commons/7/72/Favicon-Phabricator-WM.png"/></a> <a href="https://phabricator.wikimedia.org/T171740", title="T171740">Phabricator ticket</a> | <a title="By The Open Source Initiative [CC BY 2.5 (http://creativecommons.org/licenses/by/2.5)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3AOpen_Source_Initiative_keyhole.svg"><img width="16" alt="Open Source Initiative keyhole" src="https://upload.wikimedia.org/wikipedia/commons/thumb/4/4e/Open_Source_Initiative_keyhole.svg/16px-Open_Source_Initiative_keyhole.svg.png"/></a> <a href="https://github.com/wikimedia-research/Discovery-Search-Adhoc-SurveyMVP">Open source analysis</a> | <a title="Font Awesome by Dave Gandy - http://fortawesome.github.com/Font-Awesome [CC BY-SA 3.0 (http://creativecommons.org/licenses/by-sa/3.0)], via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File%3ADownload_font_awesome.svg"><img width="16" alt="Download font awesome" src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Download_font_awesome.svg/16px-Download_font_awesome.svg.png"/></a> <a href="https://github.com/wikimedia-research/Discovery-Search-Adhoc-SurveyMVP/blob/master/data">Open data</a></p>

## Background

We performed a series of tests on English Wikipedia requesting feedback from users about whether the article they were reading was relevant to one of the curated search queries. For our minimum viable product (MVP) test, we hard-coded a list of queries and articles. We also tried different wordings of the relevance question, to assess the impact of each.

![Uploaded to Phabricator by Erik Bernhardson ([F9161493](https://phabricator.wikimedia.org/F9161493))](figures/example_human_search_relevance_survey.png)

```{r data_expert}
trey <- readr::read_tsv("../data/trey_opinion.tsv")
```

For this MVP, the queries were chosen to be about topics for which we could confidently judge an article's relevance beforehand, such as American pop culture:

```{r list_queries, results='asis'}
cat(paste0("- ", paste0(unique(trey$query), collapse = "\n- ")))
```

For each query, we judged the relevance of the articles that were the top 5 results for the queries at the time (and most are still the top 5 results). The following table shows which pages we asked users about and our judgements:

```{r list_articles, results='asis'}
articles <- trey[, c("query", "article", "opinion")]
articles$query[duplicated(articles$query)] <- ""
articles$article <- paste0(
  "<a href='https://en.wikipedia.org/wiki/", gsub(" ", "_", articles$article, fixed = TRUE), "'>",
  articles$article,
  "</a>"
)
knitr::kable(articles, format = "markdown", escape = FALSE)
```

A user visiting one of those articles might be randomly picked for the survey. There were 4 varieties of questions that we asked:

1. Would you click on this page when searching for '...'?
2. If you searched for '...', would this article be a good result?
3. If you searched for '...', would this article be relevant?
4. If someone searched for '...', would they want to read this article?

(Where ... was replaced with the actual query.)

The variations on the questions were so we could assess how the wording/phrasing affected the results.

## Results

### First Test {.tabset}

```{r data_first}
responses_first <- readr::read_tsv("../data/17069968.tsv")
```
```{r aggregates_first, echo=TRUE}
aggregates_first <- responses_first %>%
  dplyr::group_by(query, article, question, choice) %>%
  dplyr::tally() %>%
  dplyr::ungroup() %>%
  tidyr::spread(choice, n, fill = 0) %>%
  dplyr::mutate(
    total = yes + no,
    score = (yes - no) / (total + 1),
    yes = yes / total,
    no = no / total,
    dismiss = dismiss / (total + dismiss),
    engaged = (total + dismiss) / (total + dismiss + timeout)
  ) %>%
  dplyr::select(-c(total, timeout)) %>%
  tidyr::gather(choice, prop, -c(query, article, question)) %>%
  dplyr::mutate(choice = factor(choice, levels = c("yes", "no", "dismiss", "engaged", "score")))
```

#### Summary

The first test (`r format(as.Date(min(responses_first$ts)), "%m/%d")`-`r format(as.Date(max(responses_first$ts)), "%m/%d")`) had 0 time delay and presented users with options to answer "Yes", "No", "I don't know", or dismiss the notification. The notification disappeared after 30 seconds if the user did not interact with it. Due to a bug, the "I don't know" responses were not recorded for this test. There were `r prettyNum(length(unique(responses_first$session_id, as.Date(responses_first$ts))), big.mark = ",")` sessions and `r prettyNum(sum(responses_first$choice %in% c("yes", "no")), big.mark = ",")` yes/no responses. `r prettyNum(sum(responses_first$choice == "timeout"), big.mark = ",")` (`r sprintf("%.1f%%", 100 * sum(responses_first$choice == "timeout") / nrow(responses_first))`) surveys timed out and `r prettyNum(sum(responses_first$choice == "dismiss"), big.mark = ",")` surveys were dismissed by the user.

```{r summary_first}
temp <- dplyr::left_join(
  tidyr::spread(aggregates_first, choice, prop),
  trey, by = c("query", "article")
)
ggplot(temp, aes(x = rating, y = score)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(formula = y ~ x, method = "lm", se = FALSE, color = "black") +
  geom_jitter(height = 0, width = 0.1, aes(color = opinion), size = 2) +
  scale_color_brewer("Trey's opinion of article's relevance", palette = "Set1") +
  facet_wrap(~ question) +
  labs(
    x = "Trey's opinion of article's relevance",
    y = "Relevance score: (#yes - #no) / (#yes + #no + 1)",
    title = "Distribution of the crowd's responses compared to expert opinion",
    subtitle = "With simple linear regression fit overlaid",
    caption = "The positive slope suggests a relationship between users' responses (as summary relevance scores) and actual page relevancy."
  ) +
  wmf::theme_facet(12, "Source Sans Pro")
ggplot(temp, aes(x = rating, y = engaged)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(formula = y ~ x, method = "lm", se = FALSE, color = "black") +
  geom_jitter(height = 0, width = 0.1, aes(color = opinion), size = 2) +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_color_brewer("Trey's opinion of article's relevance", palette = "Set1") +
  facet_wrap(~ question) +
  labs(
    x = "Trey's opinion of article's relevance",
    y = "Engagement with survey",
    title = "Users' engagement with survey (not letting it time-out)",
    subtitle = "With simple linear regression fit overlaid",
    caption = "Slight negative linear relationship suggests more users engaged with the survey (responded or dismissed) when the page was less relevant."
  ) +
  wmf::theme_facet(12, "Source Sans Pro")
rm(temp)
```

[&uarr; Top of section](#first-test)

#### Survey Responses

```{r datavis_first}
for (query in unique(aggregates_first$query)) {
  injector <- function(string) {
    return(sub("...", query, string, fixed = TRUE))
  }
  p <- ggplot(
    aggregates_first[aggregates_first$query == query & aggregates_first$choice != "score", ],
    aes(x = article, y = prop, fill = choice)
  ) +
    geom_bar(stat = "identity", position = position_dodge(width = 0.8), color = "white") +
    facet_wrap( ~ question, labeller = as_labeller(injector)) +
    scale_y_continuous(labels = scales::percent_format()) +
    scale_fill_manual("Response", values = c(
      yes = "#377EB8",
      no = "#E41A1C",
      dismiss = "#984EA3",
      engaged = "black"
    )) +
    coord_flip() +
    labs(
      title = glue("Query: '{query}'"),
      subtitle = "Yes/No is out of non-dismissive responses; dismiss % is out of all engagements; engagement is inverse of time-outs",
      x = "Article", y = "Proportion of responses"
    ) +
    wmf::theme_facet(12, "Source Sans Pro")
  print(p)
}
```

[&uarr; Top of section](#first-test)

#### Relevance Predictions

We want to be able to categorize articles as relevant/irrelevant based on user's survey responses. We train a number of classification models using expert opinion as the response and a summary *score* and *engagement* as predictors. They are computed as follows:

$$
\text{Score} = \frac{\#\{\text{response: yes}\} - \#\{\text{response: no}\}}{\#\{\text{response: yes}\} + \#\{\text{response: no}\} + 1}
$$

$$
\text{Engagement} = \frac{\#\{\text{response: yes/no/dismiss}\}}{\#\{\text{surveys}\}}
$$

The classifiers trained are:

- [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)
- [random forest](https://en.wikipedia.org/wiki/Random_forest)
- [backpropagated](https://en.wikipedia.org/wiki/Artificial_neural_network#Backpropagation) [neural network](https://en.wikipedia.org/wiki/Artificial_neural_network) with 2 hidden layers having 5 and 3 neurons, respectively
- [naïve Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)
- [gradient-boosted trees](https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting) (via [XGBoost](https://github.com/dmlc/xgboost))

We trained a classifier on each of the 4 questions using the default parameters and a random 70% of the pages as training data and assess its accuracy using a test set from the remaining 30% of the pages. We tried 5-class, 3-class, and 2-class models. The 5-label classification performed the worst due to not enough data per-class, which is why there is a substantial improvement when we grouped combined "very bad" with "bad" and combined "good" with "best" to create the 3 classes. Best performances were with binary (*irrelevant* = very bad / bad, *relevant* = ok / good / best) classification. Classifiers trained on responses to questions 1 and 4 had the highest accuracy.

```{r ratings_first, echo=TRUE, cache=TRUE}
set.seed(42)
ratings_first <- responses_first %>%
  dplyr::mutate(
    question = as.numeric(factor(question, levels = c(
      "Would you click on this page when searching for '...'?",
      "If you searched for '...', would this article be a good result?",
      "If you searched for '...', would this article be relevant?",
      "If someone searched for '...', would they want to read this article?"
    )))
  ) %>%
  dplyr::group_by(query, article, choice, question) %>%
  dplyr::tally() %>%
  dplyr::ungroup() %>%
  tidyr::spread(choice, n, fill = 0) %>%
  dplyr::mutate(
    total = yes + no + 1,
    engaged = yes + no + dismiss,
    score = (yes - no) / total,
    engagement = (engaged + dismiss) / (total + dismiss + timeout),
    # Normalized versions:
    score_norm = (score - mean(score)) / sd(score),
    engagement_norm = (engagement - mean(engagement)) / sd(engagement)
  ) %>%
  dplyr::left_join(trey, by = c("query", "article")) %>%
  dplyr::mutate(
    irrelevant = as.numeric(opinion %in% c("very bad", "bad")),
    ok_or_better = as.numeric(opinion %in% c("ok", "good", "best")),
    relevant = as.numeric(opinion %in% c("good", "best")),
    very_bad = as.numeric(opinion == "very bad"),
    bad = as.numeric(opinion == "bad"),
    ok = as.numeric(opinion == "ok"),
    good = as.numeric(opinion == "good"),
    best = as.numeric(opinion == "best"),
    opinion2 = factor(dplyr::case_when(
      opinion %in% c("very bad", "bad") ~ "bad",
      opinion %in% c("ok", "good", "best") ~ "ok or better"
    ), levels = c("bad", "ok or better")),
    opinion3 = factor(dplyr::case_when(
      opinion %in% c("very bad", "bad") ~ "bad",
      opinion %in% c("good", "best") ~ "good",
      opinion == "ok" ~ "ok"
    ), levels = c("bad", "ok", "good")),
    opinion5 = factor(opinion, levels = c("very bad", "bad", "ok", "good", "best"))
  ) %>%
  split(., .$question) %>%
  lapply(function(df) {
    training_idx <- sample.int(nrow(df), 0.7 * nrow(df), replace = FALSE)
    testing_idx <- setdiff(1:nrow(df), training_idx)
    return(list(train = df[training_idx, ], test = df[testing_idx, ]))
  })
```

```{r models_first, echo=TRUE, cache=TRUE, dependson='ratings_first'}
set.seed(0)
logistic_regression <- dplyr::bind_rows(list(
  "2" = dplyr::bind_rows(lapply(ratings_first, function(question) {
    lr <- glm(opinion2 ~ score + engagement, data = question$train, family = binomial())
    predictions <- predict(lr, question$test[, c("score", "engagement")], type = "response")
    return(data.frame(accuracy = caret::confusionMatrix(
      factor(predictions > 0.5, c(FALSE, TRUE), levels(question$test$opinion2)),
      reference = question$test$opinion2
    )$overall["Accuracy"]))
  }), .id = "question"),
  "3" = dplyr::bind_rows(lapply(ratings_first, function(question) {
    lr <- nnet::multinom(
      opinion3 ~ score + engagement,
      data = question$train,
      trace = FALSE
    )
    predictions <- predict(lr, question$test[, c("score", "engagement")])
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion3
    )$overall["Accuracy"]))
  }), .id = "question"),
  "5" = dplyr::bind_rows(lapply(ratings_first, function(question) {
    lr <- nnet::multinom(
      opinion5 ~ score + engagement,
      data = question$train,
      trace = FALSE
    )
    predictions <- predict(lr, question$test[, c("score", "engagement")])
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion5
    )$overall["Accuracy"]))
  }), .id = "question")
), .id = "categories")

random_forest <- dplyr::bind_rows(list(
  "2" = dplyr::bind_rows(lapply(ratings_first, function(question) {
    rf <- randomForest::randomForest(
      opinion2 ~ score + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions <- predict(rf, question$test[, c("score", "engagement")])
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion2
    )$overall["Accuracy"]))
  }), .id = "question"),
  "3" = dplyr::bind_rows(lapply(ratings_first, function(question) {
    rf <- randomForest::randomForest(
      opinion3 ~ score + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions <- predict(rf, question$test[, c("score", "engagement")])
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion3
    )$overall["Accuracy"]))
  }), .id = "question"),
  "5" = dplyr::bind_rows(lapply(ratings_first, function(question) {
    rf <- randomForest::randomForest(
      opinion5 ~ score + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions <- predict(rf, question$test[, c("score", "engagement")])
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion5
    )$overall["Accuracy"]))
  }), .id = "question")
), .id = "categories")

neural_net <- dplyr::bind_rows(list(
  "2" = dplyr::bind_rows(lapply(ratings_first, function(question) {
    nn <- neuralnet::neuralnet(
      irrelevant + ok_or_better ~ score + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions <- factor(
      c("bad", "ok or better")[apply(neuralnet::compute(nn, question$test[, c("score", "engagement")])$net.result, 1, which.max)],
      levels = c("bad", "ok or better")
    )
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion2
    )$overall["Accuracy"]))
  }), .id = "question"),
  "3" = dplyr::bind_rows(lapply(ratings_first, function(question) {
    nn <- neuralnet::neuralnet(
      irrelevant + ok + relevant ~ score + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions <- factor(
      c("bad", "ok", "good")[apply(neuralnet::compute(nn, question$test[, c("score", "engagement")])$net.result, 1, which.max)],
      levels = c("bad", "ok", "good")
    )
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion3
    )$overall["Accuracy"]))
  }), .id = "question"),
  "5" = dplyr::bind_rows(lapply(ratings_first, function(question) {
    nn <- neuralnet::neuralnet(
      very_bad + bad + ok + good + best ~ score + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions <- factor(
      c("very bad", "bad", "ok", "good", "best")[apply(neuralnet::compute(nn, question$test[, c("score", "engagement")])$net.result, 1, which.max)],
      levels = c("very bad", "bad", "ok", "good", "best")
    )
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion5
    )$overall["Accuracy"]))
  }), .id = "question")
), .id = "categories")

naive_bayes <- dplyr::bind_rows(list(
  "2" = dplyr::bind_rows(lapply(ratings_first, function(question) {
    nb <- e1071::naiveBayes(
      opinion2 ~ score + engagement,
      data = question$train
    )
    predictions <- predict(nb, question$test[, c("score", "engagement")])
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion2
    )$overall["Accuracy"]))
  }), .id = "question"),
  "3" = dplyr::bind_rows(lapply(ratings_first, function(question) {
    nb <- e1071::naiveBayes(
      opinion3 ~ score + engagement,
      data = question$train
    )
    predictions <- predict(nb, question$test[, c("score", "engagement")])
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion3
    )$overall["Accuracy"]))
  }), .id = "question"),
  "5" = dplyr::bind_rows(lapply(ratings_first, function(question) {
    nb <- e1071::naiveBayes(
      opinion5 ~ score + engagement,
      data = question$train
    )
    predictions <- predict(nb, question$test[, c("score", "engagement")])
    return(data.frame(accuracy = caret::confusionMatrix(
      predictions, reference = question$test$opinion5
    )$overall["Accuracy"]))
  }), .id = "question")
), .id = "categories")

gradient_boosted <- dplyr::bind_rows(list(
  "2" = dplyr::bind_rows(lapply(ratings_first, function(question) {
    xgb <- xgboost::xgboost(
      data = as.matrix(question$train[, c("score", "engagement")]),
      label = as.numeric(question$train$opinion2) - 1,
      objective = "binary:logistic", nrounds = 100, nthread = 4, verbose = 0
    )
    predictions <- predict(xgb, as.matrix(question$test[, c("score", "engagement")]))
    return(data.frame(accuracy = caret::confusionMatrix(
      factor(predictions > 0.5, c(FALSE, TRUE), c("bad", "ok or better")),
      reference = question$test$opinion2
    )$overall["Accuracy"]))
  }), .id = "question"),
  "3" = dplyr::bind_rows(lapply(ratings_first, function(question) {
    xgb <- xgboost::xgboost(
      data = as.matrix(question$train[, c("score", "engagement")]),
      label = as.numeric(question$train$opinion3) - 1, num_class = 3,
      objective = "multi:softmax", nrounds = 100, nthread = 4, verbose = 0
    )
    predictions <- predict(xgb, as.matrix(question$test[, c("score", "engagement")]))
    return(data.frame(accuracy = caret::confusionMatrix(
      factor(predictions, 0:2, levels(question$test$opinion3)),
      reference = question$test$opinion3
    )$overall["Accuracy"]))
  }), .id = "question"),
  "5" = dplyr::bind_rows(lapply(ratings_first, function(question) {
    xgb <- xgboost::xgboost(
      data = as.matrix(question$train[, c("score", "engagement")]),
      label = as.numeric(question$train$opinion5) - 1, num_class = 5,
      objective = "multi:softmax", nrounds = 100, nthread = 4, verbose = 0
    )
    predictions <- predict(xgb, as.matrix(question$test[, c("score", "engagement")]))
    return(data.frame(accuracy = caret::confusionMatrix(
      factor(predictions, 0:4, levels(question$test$opinion5)),
      reference = question$test$opinion5
    )$overall["Accuracy"]))
  }), .id = "question")
), .id = "categories")
```

```{r results_first}
DT::datatable(
  dplyr::bind_rows(list(
    "logistic regression" = logistic_regression,
    "random forest" = random_forest,
    "neural net (5, 3)" = neural_net,
    "naive bayes" = naive_bayes,
    "xgboost" = gradient_boosted
  ), .id = "classifier"),
  caption = htmltools::tags$caption(
    style = "text-align: left",
    htmltools::HTML(
      paste0(
        "Accuracy of each classifier for each of the four questions:<br>",
        paste0(c(
          "&nbsp;&nbsp;<strong>(1)</strong> Would you click on this page when searching for '...'?",
          "&nbsp;&nbsp;<strong>(2)</strong> If you searched for '...', would this article be a good result?",
          "&nbsp;&nbsp;<strong>(3)</strong> If you searched for '...', would this article be relevant?",
          "&nbsp;&nbsp;<strong>(4)</strong> If someone searched for '...', would they want to read this article?"
        ), collapse = "<br>"),
        collapse = ""
    ))
  ),
  filter = "top",
  extensions = "Buttons",
  options = list(
    pageLength = 10, autoWidth = TRUE, language = list(search = "Filter:"),
    order = list(list(4, "desc")), dom = "Bfrtip", buttons = c("copy", "csv")
  )
) %>% DT::formatPercentage("accuracy", 3)
```

[&uarr; Top of section](#first-test)

### Second Test {.tabset}

```{r data_second}
responses_second <- readr::read_tsv("../data/17073843.tsv")
```
```{r aggregates_second, echo=TRUE}
aggregates_second <- responses_second %>%
  dplyr::group_by(query, article, question, choice) %>%
  dplyr::tally() %>%
  dplyr::ungroup() %>%
  tidyr::spread(choice, n, fill = 0) %>%
  dplyr::mutate(
    total = yes + no + unsure,
    score_A = (yes - no) / (yes + no + 1),
    score_B = (yes - no + unsure / 2) / (total + 1),
    yes = yes / total,
    no = no / total,
    unsure = unsure / total,
    dismiss = dismiss / (total + dismiss),
    engaged = (total + dismiss) / (total + dismiss + timeout)
  ) %>%
  dplyr::select(-c(total, timeout)) %>%
  tidyr::gather(choice, prop, -c(query, article, question)) %>%
  dplyr::mutate(choice = factor(choice, levels = c("yes", "no", "unsure", "dismiss", "engaged", "score_A", "score_B")))
temp <- dplyr::left_join(
  tidyr::spread(aggregates_second, choice, prop),
  trey, by = c("query", "article")
)
```

#### Summary

The second test (`r format(as.Date(min(responses_second$ts)), "%m/%d")`-`r format(as.Date(max(responses_second$ts)), "%m/%d")`) **had a 60 second time delay** and presented users with options to answer "Yes"/"No"/"I don't know" (coded as "unsure") or dismiss the notification. The notification disappeared after 30 seconds if the user did not interact with it. There were `r prettyNum(length(unique(responses_second$session_id)), big.mark = ",")` sessions and `r prettyNum(sum(responses_second$choice %in% c("yes", "no", "unsure")), big.mark = ",")` yes/no/unsure responses. `r prettyNum(sum(responses_second$choice == "timeout"), big.mark = ",")` (`r sprintf("%.1f%%", 100 * sum(responses_second$choice == "timeout") / nrow(responses_second))`) surveys timed out and `r prettyNum(sum(responses_second$choice == "dismiss"), big.mark = ",")` surveys were dismissed by the user.

With the first test, it was easier to develop a scoring method using just the number of "yes" and "no" responses. With the second test, we had to include the "I don't know" (coded as "unsure") responses. Because of this, we came up with two possible scoring systems: **method A** does not count "unsure" responses but does use them to normalize the score; **method B** counts half of "unsure" responses and uses all three possible responses to normalize the score. Classifiers trained on responses to questions 3 and 4 appeared to have the highest accuracy.

$$
\text{Engagement} = \frac{\#\{\text{response: yes/no/unsure/dismiss}\}}{\#\{\text{surveys}\}}
$$

```{r dataviz_engagement}
ggplot(temp, aes(x = rating, y = engaged)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(formula = y ~ x, method = "lm", se = FALSE, color = "black") +
  geom_jitter(height = 0, width = 0.1, aes(color = opinion), size = 2) +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_color_brewer("Trey's opinion of article's relevance", palette = "Set1") +
  facet_wrap(~ question) +
  labs(
    x = "Trey's opinion of article's relevance",
    y = "Engagement with survey",
    title = "Users' engagement with survey (not letting it time-out)",
    subtitle = "With simple linear regression fit overlaid",
    caption = "Slight negative linear relationship suggests more users engaged with the survey (responded or dismissed) when the page was less relevant."
  ) +
  wmf::theme_facet(12, "Source Sans Pro")
```

[&uarr; Top of section](#second-test)

##### Scoring Method A

$$
\text{Score} = \frac{\#\{\text{response: yes}\} - \#\{\text{response: no}\}}{\#\{\text{response: yes/no}\} + 1}
$$

```{r dataviz_score_A}
ggplot(temp, aes(x = rating, y = score_A)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(formula = y ~ x, method = "lm", se = FALSE, color = "black") +
  geom_jitter(height = 0, width = 0.1, aes(color = opinion), size = 2) +
  scale_color_brewer("Trey's opinion of article's relevance", palette = "Set1") +
  facet_wrap(~ question) +
  labs(
    x = "Trey's opinion of article's relevance",
    y = "Relevance score 'A'",
    title = "Distribution of the crowd's responses compared to expert opinion",
    subtitle = "With simple linear regression fit overlaid"
  ) +
  wmf::theme_facet(12, "Source Sans Pro")
```

##### Scoring Method B

$$
\text{Score} = \frac{\#\{\text{response: yes}\} - \#\{\text{response: no}\} + \#\{\text{response: unsure}\}/2}{\#\{\text{response: yes/no/unsure}\} + 1}
$$

```{r dataviz_score_b}
ggplot(temp, aes(x = rating, y = score_B)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_smooth(formula = y ~ x, method = "lm", se = FALSE, color = "black") +
  geom_jitter(height = 0, width = 0.1, aes(color = opinion), size = 2) +
  scale_color_brewer("Trey's opinion of article's relevance", palette = "Set1") +
  facet_wrap(~ question) +
  labs(
    x = "Trey's opinion of article's relevance",
    y = "Relevance score 'B'",
    title = "Distribution of the crowd's responses compared to expert opinion",
    subtitle = "With simple linear regression fit overlaid"
  ) +
  wmf::theme_facet(12, "Source Sans Pro")
```

```{r}
rm(temp)
```

[&uarr; Top of section](#second-test)

#### Survey Responses

```{r datavis_second}
for (query in unique(aggregates_second$query)) {
  injector <- function(string) {
    return(sub("...", query, string, fixed = TRUE))
  }
  p <- ggplot(
    aggregates_second[aggregates_second$query == query & aggregates_second$choice %in% c("yes", "no", "unsure", "dismiss", "engaged"), ],
    aes(x = article, y = prop, fill = choice)
  ) +
    geom_bar(stat = "identity", position = position_dodge(width = 0.8), color = "white") +
    facet_wrap( ~ question, labeller = as_labeller(injector)) +
    scale_y_continuous(labels = scales::percent_format()) +
    scale_fill_manual("Response", values = c(
      yes = "#377EB8",
      no = "#E41A1C",
      unsure = "#FF7F00",
      dismiss = "#984EA3",
      engaged = "black"
    )) +
    coord_flip() +
    labs(
      title = glue("Query: '{query}'"),
      subtitle = "Yes/No/Unsure is out of non-dismissive responses; dismiss % is out of all engagements; engagement is inverse of time-outs",
      x = "Article", y = "Proportion of responses"
    ) +
    wmf::theme_facet(12, "Source Sans Pro")
  print(p)
}
```

[&uarr; Top of section](#second-test)

#### Relevance Predictions

```{r ratings_second, echo=TRUE, cache=TRUE}
set.seed(42)
ratings_second <- responses_second %>%
  dplyr::mutate(
    question = as.numeric(factor(question, levels = c(
      "Would you click on this page when searching for '...'?",
      "If you searched for '...', would this article be a good result?",
      "If you searched for '...', would this article be relevant?",
      "If someone searched for '...', would they want to read this article?"
    )))
  ) %>%
  dplyr::group_by(query, article, choice, question) %>%
  dplyr::tally() %>%
  dplyr::ungroup() %>%
  tidyr::spread(choice, n, fill = 0) %>%
  dplyr::mutate(
    total = yes + no + unsure + 1,
    engaged = yes + no + unsure + dismiss,
    score_A = (yes - no) / (yes + no + 1),
    score_B = (yes - no + unsure / 2) / total,
    engagement = (engaged + dismiss) / (total + dismiss + timeout),
    # Normalized versions:
    score_A_norm = (score_A - mean(score_A)) / sd(score_A),
    score_B_norm = (score_B - mean(score_B)) / sd(score_B),
    engagement_norm = (engagement - mean(engagement)) / sd(engagement)
  ) %>%
  dplyr::left_join(trey, by = c("query", "article")) %>%
  dplyr::mutate(
    irrelevant = as.numeric(opinion %in% c("very bad", "bad")),
    ok_or_better = as.numeric(opinion %in% c("ok", "good", "best")),
    relevant = as.numeric(opinion %in% c("good", "best")),
    very_bad = as.numeric(opinion == "very bad"),
    bad = as.numeric(opinion == "bad"),
    ok = as.numeric(opinion == "ok"),
    good = as.numeric(opinion == "good"),
    best = as.numeric(opinion == "best"),
    opinion2 = factor(dplyr::case_when(
      opinion %in% c("very bad", "bad") ~ "bad",
      opinion %in% c("ok", "good", "best") ~ "ok or better"
    ), levels = c("bad", "ok or better")),
    opinion3 = factor(dplyr::case_when(
      opinion %in% c("very bad", "bad") ~ "bad",
      opinion %in% c("good", "best") ~ "good",
      opinion == "ok" ~ "ok"
    ), levels = c("bad", "ok", "good")),
    opinion5 = factor(opinion, levels = c("very bad", "bad", "ok", "good", "best"))
  ) %>%
  split(., .$question) %>%
  lapply(function(df) {
    training_idx <- sample.int(nrow(df), 0.7 * nrow(df), replace = FALSE)
    testing_idx <- setdiff(1:nrow(df), training_idx)
    return(list(train = df[training_idx, ], test = df[testing_idx, ]))
  })
```

```{r models_second, echo=TRUE, cache=TRUE, dependson='ratings_second'}
data_frame <- function(...) {
  return(data.frame(..., stringsAsFactors = FALSE))
}
set.seed(0)
logistic_regression <- dplyr::bind_rows(list(
  "2" = dplyr::bind_rows(lapply(ratings_second, function(question) {
    lr_A <- glm(opinion2 ~ score_A + engagement, data = question$train, family = binomial())
    predictions_A <- predict(lr_A, question$test[, c("score_A", "engagement")], type = "response")
    lr_B <- glm(opinion2 ~ score_B + engagement, data = question$train, family = binomial())
    predictions_B <- predict(lr_B, question$test[, c("score_B", "engagement")], type = "response")
    accuracy <- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        factor(predictions_A > 0.5, c(FALSE, TRUE), levels(question$test$opinion2)),
        reference = question$test$opinion2
      )$overall["Accuracy"]),
      B = data_frame(accuracy = caret::confusionMatrix(
        factor(predictions_B > 0.5, c(FALSE, TRUE), levels(question$test$opinion2)),
        reference = question$test$opinion2
      )$overall["Accuracy"])
    )
    return(dplyr::bind_rows(accuracy, .id = "scoring"))
  }), .id = "question"),
  "3" = dplyr::bind_rows(lapply(ratings_second, function(question) {
    lr_A <- nnet::multinom(
      opinion3 ~ score_A + engagement,
      data = question$train,
      trace = FALSE
    )
    predictions_A <- predict(lr_A, question$test[, c("score_A", "engagement")])
    lr_B <- nnet::multinom(
      opinion3 ~ score_B + engagement,
      data = question$train,
      trace = FALSE
    )
    predictions_B <- predict(lr_B, question$test[, c("score_B", "engagement")])
    accuracy <- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion3
      )$overall["Accuracy"]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion3
      )$overall["Accuracy"])
    )
    return(dplyr::bind_rows(accuracy, .id = "scoring"))
  }), .id = "question"),
  "5" = dplyr::bind_rows(lapply(ratings_second, function(question) {
    lr_A <- nnet::multinom(
      opinion5 ~ score_A + engagement,
      data = question$train,
      trace = FALSE
    )
    predictions_A <- predict(lr_A, question$test[, c("score_A", "engagement")])
    lr_B <- nnet::multinom(
      opinion5 ~ score_B + engagement,
      data = question$train,
      trace = FALSE
    )
    predictions_B <- predict(lr_B, question$test[, c("score_B", "engagement")])
    accuracy <- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion5
      )$overall["Accuracy"]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion5
      )$overall["Accuracy"])
    )
    return(dplyr::bind_rows(accuracy, .id = "scoring"))
  }), .id = "question")
), .id = "categories")

random_forest <- dplyr::bind_rows(list(
  "2" = dplyr::bind_rows(lapply(ratings_second, function(question) {
    rf_A <- randomForest::randomForest(
      opinion2 ~ score_A + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions_A <- predict(rf_A, question$test[, c("score_A", "engagement")])
    rf_B <- randomForest::randomForest(
      opinion2 ~ score_B + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions_B <- predict(rf_B, question$test[, c("score_B", "engagement")])
    accuracy <- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion2
      )$overall["Accuracy"]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion2
      )$overall["Accuracy"])
    )
    return(dplyr::bind_rows(accuracy, .id = "scoring"))
  }), .id = "question"),
  "3" = dplyr::bind_rows(lapply(ratings_second, function(question) {
    rf_A <- randomForest::randomForest(
      opinion3 ~ score_A + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions_A <- predict(rf_A, question$test[, c("score_A", "engagement")])
    rf_B <- randomForest::randomForest(
      opinion3 ~ score_B + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions_B <- predict(rf_B, question$test[, c("score_B", "engagement")])
    accuracy <- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion3
      )$overall["Accuracy"]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion3
      )$overall["Accuracy"])
    )
    return(dplyr::bind_rows(accuracy, .id = "scoring"))
  }), .id = "question"),
  "5" = dplyr::bind_rows(lapply(ratings_second, function(question) {
    rf_A <- randomForest::randomForest(
      opinion5 ~ score_A + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions_A <- predict(rf_A, question$test[, c("score_A", "engagement")])
    rf_B <- randomForest::randomForest(
      opinion5 ~ score_B + engagement,
      data = question$train,
      ntree = 1000
    )
    predictions_B <- predict(rf_B, question$test[, c("score_B", "engagement")])
    accuracy <- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion5
      )$overall["Accuracy"]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion5
      )$overall["Accuracy"])
    )
    return(dplyr::bind_rows(accuracy, .id = "scoring"))
  }), .id = "question")
), .id = "categories")

neural_net <- dplyr::bind_rows(list(
  "2" = dplyr::bind_rows(lapply(ratings_second, function(question) {
    nn_A <- neuralnet::neuralnet(
      irrelevant + ok_or_better ~ score_A + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions_A <- factor(
      c("bad", "ok or better")[apply(neuralnet::compute(nn_A, question$test[, c("score_A", "engagement")])$net.result, 1, which.max)],
      levels = c("bad", "ok or better")
    )
    nn_B <- neuralnet::neuralnet(
      irrelevant + ok_or_better ~ score_B + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions_B <- factor(
      c("bad", "ok or better")[apply(neuralnet::compute(nn_B, question$test[, c("score_B", "engagement")])$net.result, 1, which.max)],
      levels = c("bad", "ok or better")
    )
    accuracy <- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion2
      )$overall["Accuracy"]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion2
      )$overall["Accuracy"])
    )
    return(dplyr::bind_rows(accuracy, .id = "scoring"))
  }), .id = "question"),
  "3" = dplyr::bind_rows(lapply(ratings_second, function(question) {
    nn_A <- neuralnet::neuralnet(
      irrelevant + ok + relevant ~ score_A + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions_A <- factor(
      c("bad", "ok", "good")[apply(neuralnet::compute(nn_A, question$test[, c("score_A", "engagement")])$net.result, 1, which.max)],
      levels = c("bad", "ok", "good")
    )
    nn_B <- neuralnet::neuralnet(
      irrelevant + ok + relevant ~ score_B + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions_B <- factor(
      c("bad", "ok", "good")[apply(neuralnet::compute(nn_B, question$test[, c("score_B", "engagement")])$net.result, 1, which.max)],
      levels = c("bad", "ok", "good")
    )
    accuracy <- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion3
      )$overall["Accuracy"]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion3
      )$overall["Accuracy"])
    )
    return(dplyr::bind_rows(accuracy, .id = "scoring"))
  }), .id = "question"),
  "5" = dplyr::bind_rows(lapply(ratings_second, function(question) {
    nn_A <- neuralnet::neuralnet(
      very_bad + bad + ok + good + best ~ score_A + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions_A <- factor(
      c("very bad", "bad", "ok", "good", "best")[apply(neuralnet::compute(nn_A, question$test[, c("score_A", "engagement")])$net.result, 1, which.max)],
      levels = c("very bad", "bad", "ok", "good", "best")
    )
    nn_B <- neuralnet::neuralnet(
      very_bad + bad + ok + good + best ~ score_B + engagement,
      data = question$train,
      hidden = c(5, 3), stepmax = 1e6
    )
    predictions_B <- factor(
      c("very bad", "bad", "ok", "good", "best")[apply(neuralnet::compute(nn_B, question$test[, c("score_A", "engagement")])$net.result, 1, which.max)],
      levels = c("very bad", "bad", "ok", "good", "best")
    )
    accuracy <- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion5
      )$overall["Accuracy"]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion5
      )$overall["Accuracy"])
    )
    return(dplyr::bind_rows(accuracy, .id = "scoring"))
  }), .id = "question")
), .id = "categories")

naive_bayes <- dplyr::bind_rows(list(
  "2" = dplyr::bind_rows(lapply(ratings_second, function(question) {
    nb_A <- e1071::naiveBayes(
      opinion2 ~ score_A + engagement,
      data = question$train
    )
    predictions_A <- predict(nb_A, question$test[, c("score_A", "engagement")])
    nb_B <- e1071::naiveBayes(
      opinion2 ~ score_B + engagement,
      data = question$train
    )
    predictions_B <- predict(nb_B, question$test[, c("score_A", "engagement")])
    accuracy <- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion2
      )$overall["Accuracy"]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion2
      )$overall["Accuracy"])
    )
    return(dplyr::bind_rows(accuracy, .id = "scoring"))
  }), .id = "question"),
  "3" = dplyr::bind_rows(lapply(ratings_second, function(question) {
    nb_A <- e1071::naiveBayes(
      opinion3 ~ score_A + engagement,
      data = question$train
    )
    predictions_A <- predict(nb_A, question$test[, c("score_A", "engagement")])
    nb_B <- e1071::naiveBayes(
      opinion3 ~ score_B + engagement,
      data = question$train
    )
    predictions_B <- predict(nb_B, question$test[, c("score_A", "engagement")])
    accuracy <- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion3
      )$overall["Accuracy"]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion3
      )$overall["Accuracy"])
    )
    return(dplyr::bind_rows(accuracy, .id = "scoring"))
  }), .id = "question"),
  "5" = dplyr::bind_rows(lapply(ratings_second, function(question) {
    nb_A <- e1071::naiveBayes(
      opinion5 ~ score_A + engagement,
      data = question$train
    )
    predictions_A <- predict(nb_A, question$test[, c("score_A", "engagement")])
    nb_B <- e1071::naiveBayes(
      opinion5 ~ score_B + engagement,
      data = question$train
    )
    predictions_B <- predict(nb_B, question$test[, c("score_A", "engagement")])
    accuracy <- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        predictions_A, reference = question$test$opinion5
      )$overall["Accuracy"]),
      B = data_frame(accuracy = caret::confusionMatrix(
        predictions_B, reference = question$test$opinion5
      )$overall["Accuracy"])
    )
    return(dplyr::bind_rows(accuracy, .id = "scoring"))
  }), .id = "question")
), .id = "categories")

gradient_boosted <- dplyr::bind_rows(list(
  "2" = dplyr::bind_rows(lapply(ratings_second, function(question) {
    xgb_A <- xgboost::xgboost(
      data = as.matrix(question$train[, c("score_A", "engagement")]),
      label = as.numeric(question$train$opinion2) - 1,
      objective = "binary:logistic", nrounds = 100, nthread = 4, verbose = 0
    )
    predictions_A <- predict(xgb_A, as.matrix(question$test[, c("score_A", "engagement")]))
    xgb_B <- xgboost::xgboost(
      data = as.matrix(question$train[, c("score_B", "engagement")]),
      label = as.numeric(question$train$opinion2) - 1,
      objective = "binary:logistic", nrounds = 100, nthread = 4, verbose = 0
    )
    predictions_B <- predict(xgb_B, as.matrix(question$test[, c("score_B", "engagement")]))
    accuracy <- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        factor(predictions_A > 0.5, c(FALSE, TRUE), c("bad", "ok or better")),
        reference = question$test$opinion2
      )$overall["Accuracy"]),
      B = data_frame(accuracy = caret::confusionMatrix(
        factor(predictions_B > 0.5, c(FALSE, TRUE), c("bad", "ok or better")),
        reference = question$test$opinion2
      )$overall["Accuracy"])
    )
    return(dplyr::bind_rows(accuracy, .id = "scoring"))
  }), .id = "question"),
  "3" = dplyr::bind_rows(lapply(ratings_second, function(question) {
    xgb_A <- xgboost::xgboost(
      data = as.matrix(question$train[, c("score_A", "engagement")]),
      label = as.numeric(question$train$opinion3) - 1, num_class = 3,
      objective = "multi:softmax", nrounds = 100, nthread = 4, verbose = 0
    )
    predictions_A <- predict(xgb_A, as.matrix(question$test[, c("score_A", "engagement")]))
    xgb_B <- xgboost::xgboost(
      data = as.matrix(question$train[, c("score_B", "engagement")]),
      label = as.numeric(question$train$opinion3) - 1, num_class = 3,
      objective = "multi:softmax", nrounds = 100, nthread = 4, verbose = 0
    )
    predictions_B <- predict(xgb_B, as.matrix(question$test[, c("score_B", "engagement")]))
    accuracy <- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        factor(predictions_A, 0:2, levels(question$test$opinion3)),
        reference = question$test$opinion3
      )$overall["Accuracy"]),
      B = data_frame(accuracy = caret::confusionMatrix(
        factor(predictions_B, 0:2, levels(question$test$opinion3)),
        reference = question$test$opinion3
      )$overall["Accuracy"])
    )
    return(dplyr::bind_rows(accuracy, .id = "scoring"))
  }), .id = "question"),
  "5" = dplyr::bind_rows(lapply(ratings_second, function(question) {
    xgb_A <- xgboost::xgboost(
      data = as.matrix(question$train[, c("score_A", "engagement")]),
      label = as.numeric(question$train$opinion5) - 1, num_class = 5,
      objective = "multi:softmax", nrounds = 100, nthread = 4, verbose = 0
    )
    predictions_A <- predict(xgb_A, as.matrix(question$test[, c("score_A", "engagement")]))
    xgb_B <- xgboost::xgboost(
      data = as.matrix(question$train[, c("score_B", "engagement")]),
      label = as.numeric(question$train$opinion5) - 1, num_class = 5,
      objective = "multi:softmax", nrounds = 100, nthread = 4, verbose = 0
    )
    predictions_B <- predict(xgb_B, as.matrix(question$test[, c("score_B", "engagement")]))
    accuracy <- list(
      A = data_frame(accuracy = caret::confusionMatrix(
        factor(predictions_A, 0:4, levels(question$test$opinion5)),
        reference = question$test$opinion5
      )$overall["Accuracy"]),
      B = data_frame(accuracy = caret::confusionMatrix(
        factor(predictions_B, 0:4, levels(question$test$opinion5)),
        reference = question$test$opinion5
      )$overall["Accuracy"])
    )
    return(dplyr::bind_rows(accuracy, .id = "scoring"))
  }), .id = "question")
), .id = "categories")
```

```{r results_second}
DT::datatable(
  dplyr::bind_rows(list(
    "logistic regression" = logistic_regression,
    "random forest" = random_forest,
    "neural net (5, 3)" = neural_net,
    "naive bayes" = naive_bayes,
    "xgboost" = gradient_boosted
  ), .id = "classifier"),
  caption = htmltools::tags$caption(
    style = "text-align: left",
    htmltools::HTML(
      paste0(
        "Accuracy of each classifier for each of the four questions:<br>",
        paste0(c(
          "&nbsp;&nbsp;<strong>(1)</strong> Would you click on this page when searching for '...'?",
          "&nbsp;&nbsp;<strong>(2)</strong> If you searched for '...', would this article be a good result?",
          "&nbsp;&nbsp;<strong>(3)</strong> If you searched for '...', would this article be relevant?",
          "&nbsp;&nbsp;<strong>(4)</strong> If someone searched for '...', would they want to read this article?"
        ), collapse = "<br>"),
        collapse = ""
    ))
  ),
  filter = "top",
  extensions = "Buttons",
  options = list(
    pageLength = 10, autoWidth = TRUE, language = list(search = "Filter:"),
    order = list(list(5, "desc")), dom = "Bfrtip", buttons = c("copy", "csv")
  )
) %>% DT::formatPercentage("accuracy", 3)
```

[&uarr; Top of section](#second-test)

## Conclusion

Using relatively few article judgements, we are able to train models that perform remarkably well given the amount of data. That is, using a larger set of articles for which we have an "expert opinion" regarding their relevance to a certain search query, we will be able to train a model that accurately predicts an article's relevance just from users' survey responses and engagement with the survey. This will enable us to use aggregated public opinion to rank a large volume of articles.

Using binary classification (irrelevant/relevant), we can ask the algorithms to return predicted probabilities instead of predicted classes. Assuming the model has exceptional accuracy, we can then feed queries and rankings (predicted from users' survey responses) as training data into our [learning-to-rank (LTR)](https://en.wikipedia.org/wiki/Learning_to_rank) endeavor.[^announcement]

In a presentation of this work,[^notes] [Dario Taraborelli](https://meta.wikimedia.org/wiki/User:Dario_(WMF)) (Head of [Wikimedia Research](https://www.mediawiki.org/wiki/Wikimedia_Research)) brought up the valid point of how the question wording ("relevant to you" vs. "relevant to people") primes the respondent differently and could yield different results. It was interesting, then, to see question 4 ("If _**someone**_ searched for '...', would _**they**_ want to read this article?") show up in both tests as the question whose responses yielded the better classification performances.

[^announcement]: [Wikimedia Engineering/June 2017 changes/Update on Discovery](https://www.mediawiki.org/wiki/Wikimedia_Engineering/June_2017_changes/Update_on_Discovery)

[^notes]: Links for [notes from Research Group meeting on 24 August 2017](https://phabricator.wikimedia.org/T171740#3549858) and the [slide deck presented](https://docs.google.com/a/wikimedia.org/presentation/d/1PuOOSukPYFGWikppGmw9Cg85fay-IT9Fi8gB8-CUda8/edit?usp=sharing)

## Discussion

As this was a proof of concept / MVP, we did not apply a lot of rigor to certain aspects. For example, we did not have multiple experts agree on the articles' relevance, but rather one person's opinions were used as the gold standard. On the analysis side, a lot of the time was spent getting multiple classification algorithms running in different configuration combinations; so we were not able to include a more rigorous accuracy estimation approach like cross-validation. At the time of this report's publication we are planning on launching a third test ([T174106](https://phabricator.wikimedia.org/T174106)).

Then there's the issue of what this looks like in production. For deploying this on Wikipedia, we might want to specifically optimize for encyclopedic searches. For example, "barack obama birthdate" and "iphone 7 release date" are encyclopedic search queries, but "how do i 3d print a miniature santa claus?" is not. Unless we utilize natural language processing and machine learning to detect such queries, we cannot automate query selection by picking the most popular queries to ask users about. We would also need to decide whether to train relevance models on a per-wiki/project/language basis. That is, we cannot assume that the relationship between survey responses, engagement, and relevance is the same on German Wikipedia, French Wikisource, and Wikidata.

Filtering for encyclopedicity is just one thing that human reviewers will have to do. We will have to curate the queries anyway because we need to filter for personally identifiable information (PII), gibberish, and queries in the wrong language. However, although manually reviewing 500 queries takes a few hours, we can easily leverage that into 25K-50K crowd-sourced relevance judgements (50-100 results per query) in a relatively short time, whereas the more tedious [Discernatron](https://www.mediawiki.org/wiki/Discernatron) has only gotten ~7500 (150 queries * ~50 results per query) over the course of about a year.
